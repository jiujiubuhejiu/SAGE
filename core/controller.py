"""
SAGE Controller - Main controller integrating 3-layer architecture
Integrates state machine, prompt generator, and output validator
"""

import time
import re
from typing import Dict, Any, Optional, Tuple, List
from core.state_machine import SAGEStateMachine, SAGEState, Hypothesis, TestResult, Exemplar
from tools.prompt_generator import PromptGenerator
from tools.output_validator import OutputValidator
from core.agent import ask_agent, validate_agent_response


class SAGEController:
    """SAGE main controller - integrates 3-layer architecture."""
    
    def __init__(self, feature_id: int, layer: int, llm_client, tools, experiment_env,
                 debug: bool = False, max_rounds: int = 30, top_k: int = 10):
        self.feature_id = feature_id
        self.layer = layer
        self.llm_client = llm_client
        self.tools = tools
        self.experiment_env = experiment_env
        self.debug = debug
        self.top_k = top_k
        
        # Initialize 3-layer architecture
        self.state_machine = SAGEStateMachine(feature_id, layer, max_rounds)
        self.prompt_generator = PromptGenerator(self.state_machine, top_k=self.top_k)
        self.output_validator = OutputValidator(top_k=self.top_k)
        
        # Execution statistics
        self.execution_stats = {
            "total_rounds": 0,
            "successful_rounds": 0,
            "failed_rounds": 0,
            "retry_attempts": 0,
            "start_time": None,
            "end_time": None
        }
        
        # Loop detection
        self.consecutive_failures = 0
        self.max_consecutive_failures = 5
    
    def run(self) -> Dict[str, Any]:
        """Main execution loop."""
        self.execution_stats["start_time"] = time.time()
        
        if self.debug:
            print(f"ğŸš€ Starting SAGE Controller for Feature {self.feature_id} at Layer {self.layer}")
        
        try:
            while not self.state_machine.is_final_state():
                self._execute_round()
                
                # Safety check
                if self.state_machine.round > 20:
                    self._force_conclude()
                    break
            
            self.execution_stats["end_time"] = time.time()
            return self._compile_results()
            
        except Exception as e:
            if self.debug:
                print(f"âŒ Controller error: {e}")
            self._force_conclude()
            return self._compile_results()
    
    def _execute_round(self):
        """æ‰§è¡Œå•è½®åˆ†æ"""
        self.execution_stats["total_rounds"] += 1
        current_state = self.state_machine.state
        
        if self.debug:
            print(f"\n--- Round {self.state_machine.round} ---")
            print(f"State: {current_state.value}")
        
        # Round 0: è‡ªåŠ¨è½¬æ¢åˆ°GET_EXEMPLARSï¼Œä¸ä½¿ç”¨LLM
        if current_state == SAGEState.INIT:
            self.state_machine.transition(SAGEState.GET_EXEMPLARS)
            return True
        
        # Round 1: è‡ªåŠ¨æ‰§è¡ŒGET_EXEMPLARSï¼Œä¸ä½¿ç”¨LLM
        if current_state == SAGEState.GET_EXEMPLARS:
            return self._auto_execute_get_exemplars()
        
        # PARALLEL_HYPOTHESIS_TESTING: å¹¶è¡Œå‡è®¾å¤„ç†å…¥å£
        if current_state == SAGEState.PARALLEL_HYPOTHESIS_TESTING:
            return self._execute_parallel_hypothesis_testing()
        
        # DESIGN_TEST: ä»…ç”¨äºéå¹¶è¡Œæ¨¡å¼çš„æ—§é€»è¾‘ï¼ˆå·²åºŸå¼ƒï¼Œä¿ç•™ç”¨äºå…¼å®¹æ€§ï¼‰
        # In ... modeï¼ŒDESIGN_TESTç”±_process_hypothesis_design_testå¤„ç†
        if current_state == SAGEState.DESIGN_TEST:
            # Checkæ˜¯å¦åœ¨å¹¶è¡Œæ¨¡å¼ä¸‹
            if self.state_machine.current_hypothesis_id:
                # å¹¶è¡Œæ¨¡å¼ä¸‹ä¸åº”è¯¥ç›´æ¥è¿›å…¥DESIGN_TESTï¼Œè¿™åº”è¯¥é€šè¿‡PARALLEL_HYPOTHESIS_TESTINGå¤„ç†
                if self.debug:
                    print("âš ï¸  DESIGN_TEST state in parallel mode, redirecting to PARALLEL_HYPOTHESIS_TESTING")
                self.state_machine.transition(SAGEState.PARALLEL_HYPOTHESIS_TESTING)
                return True
            else:
                # éå¹¶è¡Œæ¨¡å¼ï¼Œä½¿ç”¨æ—§é€»è¾‘
                return self._execute_design_test_with_immediate_run()
        
        # å…¶ä»–è½®æ¬¡ä½¿ç”¨LLM
        # 1. ç”Ÿæˆå½“å‰çŠ¶æ€çš„Prompt
        print(f"ğŸ”„ Generating prompt for state: {current_state.value}")
        prompt = self.prompt_generator.generate()
        
        print(f"âœ… Generated prompt ({len(prompt)} chars)")
        if self.debug:
            print(f"   Prompt preview: {prompt[:200]}...")
        
        # 2. è°ƒç”¨LLM (å¸¦é‡è¯•æœºåˆ¶)
        print(f"ğŸ¤– Calling LLM (this may take a while, especially if API key is not set)...")
        llm_output = self._get_llm_response_with_retry(prompt)
        print(f"âœ… Received LLM response ({len(llm_output)} chars)")
        
        # 3. éªŒè¯è¾“å‡º
        # Checkæ˜¯å¦è¾¾åˆ°æœ€å¤§round
        is_max_round = self.state_machine.round >= self.state_machine.max_rounds
        is_valid, error_msg = self.output_validator.validate(current_state, llm_output, is_max_round=is_max_round)
        
        if not is_valid:
            if self.debug:
                print(f"âš ï¸  Validation failed: {error_msg}")
            
            # Ifè¾¾åˆ°æœ€å¤§roundä¸”åœ¨FINAL_CONCLUSIONçŠ¶æ€ï¼Œå¼ºåˆ¶æ¥å—è¾“å‡º
            if is_max_round and current_state == SAGEState.FINAL_CONCLUSION:
                print(f"âš ï¸  Max round reached: Accepting conclusion despite validation warnings")
                is_valid = True  # Forceæ¥å—
            else:
                # é‡è¯•é€»è¾‘
                llm_output = self._retry_with_correction(prompt, error_msg)
                
                # å†æ¬¡éªŒè¯
                is_valid, error_msg = self.output_validator.validate(current_state, llm_output, is_max_round=is_max_round)
                if not is_valid:
                    # Ifè¾¾åˆ°æœ€å¤§roundä¸”åœ¨FINAL_CONCLUSIONçŠ¶æ€ï¼Œå¼ºåˆ¶æ¥å—
                    if is_max_round and current_state == SAGEState.FINAL_CONCLUSION:
                        print(f"âš ï¸  Max round reached: Accepting conclusion despite validation warnings")
                        is_valid = True
                    else:
                        self.consecutive_failures += 1
                        if self.consecutive_failures >= self.max_consecutive_failures:
                            if self.debug:
                                print(f"ğŸ›‘ Too many consecutive failures ({self.consecutive_failures}). Forcing conclusion.")
                            self._force_conclude()
                            return
                        self._handle_persistent_error(error_msg)
                        return
        
        # 4. å¤„ç†è¾“å‡º
        self._process_output(current_state, llm_output)
        
        # 5. çŠ¶æ€è½¬æ¢
        next_state = self._determine_next_state()
        self.state_machine.transition(next_state)
        
        self.execution_stats["successful_rounds"] += 1
        self.consecutive_failures = 0  # Resetå¤±è´¥è®¡æ•°
        
        if self.debug:
            print(f"âœ… Round completed, transitioning to {next_state.value}")
    
    def _auto_execute_get_exemplars(self):
        """è‡ªåŠ¨æ‰§è¡ŒGET_EXEMPLARSï¼Œä¸ä½¿ç”¨LLM"""
        if self.debug:
            print("ğŸ¤– Auto-executing GET_EXEMPLARS...")
        
        try:
            # Resetå®éªŒç¯å¢ƒçš„text_exemplars_calledæ ‡å¿—
            self.experiment_env.text_exemplars_called = False
            
            # ç›´æ¥è°ƒç”¨å®éªŒç¯å¢ƒè·å–exemplars
            tool_output = self.experiment_env.execute_experiment(f"[TOOL] text_exemplars top_k={self.top_k}")
            
            if self.debug:
                print(f"ğŸ“Š Tool execution result:")
                print(tool_output)
            
            # Processå·¥å…·è¾“å‡ºï¼Œæ›´æ–°çŠ¶æ€æœº
            success = self._process_tool_output("text_exemplars", tool_output)
            
            if success:
                # çŠ¶æ€è½¬æ¢åˆ°ANALYZE_EXEMPLARS
                self.state_machine.transition(SAGEState.ANALYZE_EXEMPLARS)
                
                if self.debug:
                    print("âœ… Auto-execution completed, transitioning to ANALYZE_EXEMPLARS")
                
                self.execution_stats["successful_rounds"] += 1
                return True
            else:
                if self.debug:
                    print("âŒ Tool output processing failed")
                return False
            
        except Exception as e:
            if self.debug:
                print(f"âŒ Auto-execution failed: {e}")
            return False
    
    def _process_tool_output(self, tool_name: str, tool_output: str):
        """å¤„ç†å·¥å…·è¾“å‡ºï¼Œæ›´æ–°çŠ¶æ€æœº"""
        if tool_name == "text_exemplars":
            # Parseexemplarsè¾“å‡ºå¹¶æ›´æ–°çŠ¶æ€æœº
            if "ERROR:" in tool_output:
                print(f"âš ï¸  Tool error: {tool_output}")
                return False
            
            # Priorityä»experiment_envè·å–è¯¦ç»†çš„exemplarsæ•°æ®ï¼ˆåŒ…å«å®Œæ•´tokensåºåˆ—ï¼‰
            try:
                # Checkæ˜¯å¦æœ‰ last_detailed_exemplars å±æ€§
                has_attr = hasattr(self.experiment_env, 'last_detailed_exemplars')
                if has_attr:
                    last_exemplars = self.experiment_env.last_detailed_exemplars
                    print(f"ğŸ” Debug: has last_detailed_exemplars={has_attr}, length={len(last_exemplars) if last_exemplars else 0}")
                    
                    if last_exemplars and len(last_exemplars) > 0:
                        # ç›´æ¥ä»è¯¦ç»†æ•°æ®åˆ›å»ºExemplarå¯¹è±¡
                        exemplars = []
                        for ex_dict in last_exemplars:
                            exemplar = Exemplar(
                                text=ex_dict.get("text", ""),
                                activation=ex_dict.get("max_activation", 0.0),
                                tokens=ex_dict.get("tokens", []),
                                per_token_activations=ex_dict.get("per_token_activations", [])
                            )
                            exemplars.append(exemplar)
                        
                        if exemplars:
                            self.state_machine.set_exemplars(exemplars)
                            print(f"ğŸ“Š Stored {len(exemplars)} exemplars with full token data to state machine")
                            return True
                        else:
                            print(f"âš ï¸  Failed to create exemplars from last_detailed_exemplars (empty list after processing)")
                    else:
                        print(f"âš ï¸  last_detailed_exemplars is empty or None (length={len(last_exemplars) if last_exemplars else 0})")
                else:
                    print(f"âš ï¸  experiment_env does not have last_detailed_exemplars attribute")
                
                # Fallback: ä»æ–‡æœ¬è¾“å‡ºè§£æï¼ˆå¦‚æœè¯¦ç»†æ•°æ®ä¸å¯ç”¨ï¼‰
                print(f"ğŸ”„ Falling back to parsing exemplars from text output...")
                exemplars = self._parse_exemplars_from_output(tool_output)
                if exemplars:
                    self.state_machine.set_exemplars(exemplars)
                    print(f"ğŸ“Š Stored {len(exemplars)} exemplars (parsed from text) to state machine")
                else:
                    print("âš ï¸  No exemplars parsed from output")
            except Exception as e:
                print(f"âŒ Error parsing exemplars: {e}")
                import traceback
                traceback.print_exc()
                return False
            
            return True
        
        elif tool_name == "model.run":
            # Processmodel.runè¾“å‡º
            if self.debug:
                print(f"ğŸ“Š Processing {tool_name} output")
            return True
        
        return True
    
    def _execute_parallel_hypothesis_testing(self):
        """æ‰§è¡Œå¹¶è¡Œå‡è®¾æµ‹è¯•
        ä¸ºæ‰€æœ‰æ´»è·ƒå‡è®¾åŒæ—¶æ‰§è¡Œä¸€ä¸ªæ­¥éª¤ï¼ˆDESIGN_TEST/ANALYZE_RESULT/UPDATE_HYPOTHESISï¼‰
        æ¯ä¸ªå‡è®¾ç‹¬ç«‹ç»´æŠ¤è‡ªå·±çš„çŠ¶æ€å’Œå¾ªç¯
        """
        # Checkæ˜¯å¦æœ‰è¡¥å……æµ‹è¯•éœ€è¦æ‰§è¡Œï¼ˆæ¥è‡ªREVIEWï¼‰
        if hasattr(self.state_machine, 'supplemental_tests') and self.state_machine.supplemental_tests:
            if self.debug:
                print(f"ğŸ”¬ Executing {len(self.state_machine.supplemental_tests)} supplemental tests from REVIEW")
            self._execute_supplemental_tests()
            # Clearè¡¥å……æµ‹è¯•åˆ—è¡¨
            self.state_machine.supplemental_tests = []
            # è¿”å›REVIEWæŸ¥çœ‹æ–°æµ‹è¯•ç»“æœ
            self.state_machine.transition(SAGEState.REVIEW_ALL_HYPOTHESES)
            return True

        # Checkæ˜¯å¦æ‰€æœ‰å‡è®¾éƒ½å·²æœ€ç»ˆç¡®å®š
        if self.state_machine.all_hypotheses_finalized():
            if self.debug:
                print("âœ… All hypotheses finalized, transitioning to REVIEW_ALL_HYPOTHESES")
            self.state_machine.transition(SAGEState.REVIEW_ALL_HYPOTHESES)
            return True
        
        # Getæ‰€æœ‰æ´»è·ƒå‡è®¾
        active_hypotheses = self.state_machine.get_active_hypotheses()
        
        if not active_hypotheses:
            if self.debug:
                print("âš ï¸  No active hypotheses found, transitioning to REVIEW_ALL_HYPOTHESES")
            self.state_machine.transition(SAGEState.REVIEW_ALL_HYPOTHESES)
            return True
        
        if self.debug:
            print(f"ğŸ”„ Parallel Processing Round: Processing {len(active_hypotheses)} active hypotheses")
            for hyp in active_hypotheses:
                state_str = hyp.current_state.value if hyp.current_state else "COMPLETED"
                print(f"   H{hyp.id}: {state_str} (Status: {hyp.status}, Tests: {len(hyp.test_history)})")
        
        # æŒ‰å‡è®¾IDæ’åºï¼Œç¡®ä¿å¤„ç†é¡ºåºä¸€è‡´
        active_hypotheses.sort(key=lambda h: h.id)
        
        # ä¸ºæ¯ä¸ªæ´»è·ƒå‡è®¾æ‰§è¡Œä¸€ä¸ªæ­¥éª¤
        for hypothesis in active_hypotheses:
            # Skipå·²å®Œæˆçš„å‡è®¾
            if hypothesis.status in ["CONFIRMED", "REFUTED"]:
                continue
            
            # Ensureå‡è®¾æœ‰current_stateï¼Œå¦‚æœæ²¡æœ‰åˆ™åˆå§‹åŒ–ä¸ºDESIGN_TEST
            if hypothesis.current_state is None:
                hypothesis.current_state = SAGEState.DESIGN_TEST
            
            # According toå‡è®¾çš„å½“å‰çŠ¶æ€æ‰§è¡Œç›¸åº”æ“ä½œ
            try:
                if hypothesis.current_state == SAGEState.DESIGN_TEST:
                    self._process_hypothesis_design_test(hypothesis)
                elif hypothesis.current_state == SAGEState.ANALYZE_RESULT:
                    self._process_hypothesis_analyze_result(hypothesis)
                elif hypothesis.current_state == SAGEState.UPDATE_HYPOTHESIS:
                    self._process_hypothesis_update(hypothesis)
                else:
                    # Not ...
                    if self.debug:
                        print(f"âš ï¸  Unknown state for H{hypothesis.id}, resetting to DESIGN_TEST")
                    hypothesis.current_state = SAGEState.DESIGN_TEST
                    self._process_hypothesis_design_test(hypothesis)
            except Exception as e:
                if self.debug:
                    print(f"âŒ Error processing H{hypothesis.id}: {e}")
                # å‘ç”Ÿé”™è¯¯æ—¶ï¼Œé‡ç½®ä¸ºDESIGN_TEST
                hypothesis.current_state = SAGEState.DESIGN_TEST
        
        # Allå‡è®¾å¤„ç†å®Œæˆåï¼Œç»§ç»­ä¿æŒåœ¨PARALLEL_HYPOTHESIS_TESTINGçŠ¶æ€
        # ä¸‹ä¸€è½®ä¼šè‡ªåŠ¨å†æ¬¡å¤„ç†æ‰€æœ‰æ´»è·ƒå‡è®¾
        return True
    
    def _process_hypothesis_design_test(self, hypothesis: Hypothesis):
        """ä¸ºå•ä¸ªå‡è®¾æ‰§è¡ŒDESIGN_TESTæ­¥éª¤"""
        if self.debug:
            print(f"\n{'='*60}")
            print(f"ğŸ“‹ Processing H{hypothesis.id} - DESIGN_TEST")
            print(f"   Hypothesis: {hypothesis.text[:80]}...")
            print(f"{'='*60}")
        
        # Setå½“å‰å‡è®¾ID
        old_current_id = self.state_machine.current_hypothesis_id
        self.state_machine.current_hypothesis_id = hypothesis.id
        
        try:
            # 1. ç”ŸæˆDESIGN_TESTçš„prompt
            # Temporarily set state to DESIGN_TEST for prompt generation
            old_state = self.state_machine.state
            self.state_machine.state = SAGEState.DESIGN_TEST
            prompt = self.prompt_generator.generate()
            self.state_machine.state = old_state  # Restore state

            if self.debug:
                print(f"   Generated prompt ({len(prompt)} chars)")
            
            # 2. è°ƒç”¨LLMè·å–æµ‹è¯•è®¾è®¡ï¼ˆæ·»åŠ å‡è®¾æ ‡è¯†ï¼Œé¿å…æ··æ·†ï¼‰
            prompt_with_id = f"[DESIGNING TEST FOR HYPOTHESIS {hypothesis.id} ONLY]\n{prompt}"
            llm_output = self._get_llm_response_with_retry(prompt_with_id)
            
            # 3. éªŒè¯è¾“å‡º
            is_valid, error_msg = self.output_validator.validate(SAGEState.DESIGN_TEST, llm_output)
            if not is_valid:
                if self.debug:
                    print(f"   âš ï¸  Validation failed: {error_msg}")
                llm_output = self._retry_with_correction(prompt, error_msg)
                is_valid, error_msg = self.output_validator.validate(SAGEState.DESIGN_TEST, llm_output)
                if not is_valid:
                    if self.debug:
                        print(f"   âŒ Persistent validation error: {error_msg}")
                    hypothesis.current_state = SAGEState.DESIGN_TEST  # Keepå½“å‰çŠ¶æ€ï¼Œä¸‹æ¬¡é‡è¯•
                    return
            
            # 4. å¤„ç†è¾“å‡ºå¹¶æ‰§è¡Œæµ‹è¯•
            self._process_output(SAGEState.DESIGN_TEST, llm_output)
            
            # 5. æ‰§è¡Œæµ‹è¯•ï¼ˆå¦‚æœåŒ…å«[TOOL] model.runï¼‰
            if "[TOOL] model.run" in llm_output:
                test_prompt = self._extract_test_prompt_from_design(llm_output)
                if test_prompt:
                    self._execute_test_immediately(test_prompt, hypothesis_id=hypothesis.id)
            
            # 6. æ›´æ–°å‡è®¾çŠ¶æ€ä¸ºANALYZE_RESULT
            hypothesis.current_state = SAGEState.ANALYZE_RESULT
            
            if self.debug:
                print(f"   âœ… H{hypothesis.id} DESIGN_TEST completed, moving to ANALYZE_RESULT")
        
        finally:
            # Restoreä¹‹å‰çš„current_hypothesis_id
            self.state_machine.current_hypothesis_id = old_current_id
    
    def _process_hypothesis_analyze_result(self, hypothesis: Hypothesis):
        """ä¸ºå•ä¸ªå‡è®¾æ‰§è¡ŒANALYZE_RESULTæ­¥éª¤"""
        if self.debug:
            print(f"\n{'='*60}")
            print(f"ğŸ“Š Processing H{hypothesis.id} - ANALYZE_RESULT")
            print(f"   Hypothesis: {hypothesis.text[:80]}...")
            print(f"   Test history: {len(hypothesis.test_history)} tests")
            if hypothesis.latest_test_execution_output:
                print(f"   âœ… Test execution output available ({len(hypothesis.latest_test_execution_output)} chars)")
            print(f"{'='*60}")
        
        # Checkæ˜¯å¦æœ‰æµ‹è¯•ç»“æœ
        if not hypothesis.test_history and not hypothesis.latest_test_execution_output:
            if self.debug:
                print(f"   âš ï¸  No test results for H{hypothesis.id}, resetting to DESIGN_TEST")
            hypothesis.current_state = SAGEState.DESIGN_TEST
            return
        
        # Setå½“å‰å‡è®¾ID
        old_current_id = self.state_machine.current_hypothesis_id
        self.state_machine.current_hypothesis_id = hypothesis.id
        
        try:
            # 1. ç”ŸæˆANALYZE_RESULTçš„prompt
            # Temporarily set state to ANALYZE_RESULT for prompt generation
            old_state = self.state_machine.state
            self.state_machine.state = SAGEState.ANALYZE_RESULT
            prompt = self.prompt_generator.generate()
            self.state_machine.state = old_state  # Restore state

            if self.debug:
                print(f"   Generated prompt ({len(prompt)} chars)")
                # Checkpromptä¸­æ˜¯å¦åŒ…å«æµ‹è¯•ç»“æœ
                if "Complete Test Execution Output" in prompt or "Test Result" in prompt:
                    print(f"   âœ… Prompt contains test execution output")
                else:
                    print(f"   âš ï¸  Warning: Prompt may not contain test execution output")
            
            # 2. è°ƒç”¨LLMåˆ†æç»“æœï¼ˆæ·»åŠ å‡è®¾æ ‡è¯†åˆ°promptï¼Œé¿å…æ··æ·†ï¼‰
            prompt_with_id = f"[ANALYZING HYPOTHESIS {hypothesis.id} ONLY]\n{prompt}"
            llm_output = self._get_llm_response_with_retry(prompt_with_id)
            
            # 3. éªŒè¯è¾“å‡º
            is_valid, error_msg = self.output_validator.validate(SAGEState.ANALYZE_RESULT, llm_output)
            if not is_valid:
                if self.debug:
                    print(f"   âš ï¸  Validation failed: {error_msg}")
                llm_output = self._retry_with_correction(prompt, error_msg)
                is_valid, error_msg = self.output_validator.validate(SAGEState.ANALYZE_RESULT, llm_output)
                if not is_valid:
                    if self.debug:
                        print(f"   âŒ Persistent validation error: {error_msg}")
                    hypothesis.current_state = SAGEState.ANALYZE_RESULT  # Keepå½“å‰çŠ¶æ€ï¼Œä¸‹æ¬¡é‡è¯•
                    return
            
            # 4. å¤„ç†è¾“å‡º
            self._process_output(SAGEState.ANALYZE_RESULT, llm_output)
            
            # 5. æ›´æ–°å‡è®¾çŠ¶æ€ä¸ºUPDATE_HYPOTHESIS
            hypothesis.current_state = SAGEState.UPDATE_HYPOTHESIS
            
            if self.debug:
                print(f"   âœ… H{hypothesis.id} ANALYZE_RESULT completed, moving to UPDATE_HYPOTHESIS")
        
        finally:
            # Restoreä¹‹å‰çš„current_hypothesis_id
            self.state_machine.current_hypothesis_id = old_current_id
    
    def _process_hypothesis_update(self, hypothesis: Hypothesis):
        """ä¸ºå•ä¸ªå‡è®¾æ‰§è¡ŒUPDATE_HYPOTHESISæ­¥éª¤"""
        if self.debug:
            print(f"\n{'='*60}")
            print(f"ğŸ“ Processing H{hypothesis.id} - UPDATE_HYPOTHESIS")
            print(f"   Hypothesis: {hypothesis.text[:80]}...")
            print(f"{'='*60}")
        
        # Setå½“å‰å‡è®¾ID
        old_current_id = self.state_machine.current_hypothesis_id
        self.state_machine.current_hypothesis_id = hypothesis.id
        
        try:
            # 1. ç”ŸæˆUPDATE_HYPOTHESISçš„prompt
            # Temporarily set state to UPDATE_HYPOTHESIS for prompt generation
            old_state = self.state_machine.state
            self.state_machine.state = SAGEState.UPDATE_HYPOTHESIS
            prompt = self.prompt_generator.generate()
            self.state_machine.state = old_state  # Restore state

            if self.debug:
                print(f"   Generated prompt ({len(prompt)} chars)")
            
            # 2. è°ƒç”¨LLMæ›´æ–°å‡è®¾ï¼ˆæ·»åŠ å‡è®¾æ ‡è¯†ï¼Œé¿å…æ··æ·†ï¼‰
            prompt_with_id = f"[UPDATING HYPOTHESIS {hypothesis.id} ONLY]\n{prompt}"
            llm_output = self._get_llm_response_with_retry(prompt_with_id)
            
            # 3. éªŒè¯è¾“å‡º
            is_valid, error_msg = self.output_validator.validate(SAGEState.UPDATE_HYPOTHESIS, llm_output)
            if not is_valid:
                if self.debug:
                    print(f"   âš ï¸  Validation failed: {error_msg}")
                llm_output = self._retry_with_correction(prompt, error_msg)
                is_valid, error_msg = self.output_validator.validate(SAGEState.UPDATE_HYPOTHESIS, llm_output)
                if not is_valid:
                    if self.debug:
                        print(f"   âŒ Persistent validation error: {error_msg}")
                    hypothesis.current_state = SAGEState.UPDATE_HYPOTHESIS  # Keepå½“å‰çŠ¶æ€ï¼Œä¸‹æ¬¡é‡è¯•
                    return
            
            # 4. å¤„ç†è¾“å‡º
            self._process_output(SAGEState.UPDATE_HYPOTHESIS, llm_output)
            
            # 5. è§£æå‡è®¾æ›´æ–°ï¼Œç¡®å®šä¸‹ä¸€ä¸ªçŠ¶æ€
            hypothesis_updates = self._parse_hypothesis_updates(llm_output)
            for hyp_update in hypothesis_updates:
                if hyp_update.get("hypothesis_id") == hypothesis.id:
                    status = hyp_update.get("status")
                    if status in ["CONFIRMED", "REFUTED"]:
                        # å‡è®¾å·²å®Œæˆï¼Œæ¸…é™¤current_state
                        hypothesis.current_state = None
                        if self.debug:
                            print(f"   âœ… H{hypothesis.id} {status}, stopping cycle")
                    else:
                        # Continueæµ‹è¯•ï¼Œå›åˆ°DESIGN_TEST
                        hypothesis.current_state = SAGEState.DESIGN_TEST
                        if self.debug:
                            print(f"   âœ… H{hypothesis.id} {status}, continuing to DESIGN_TEST")
                    break
            else:
                # Ifæ²¡æœ‰æ‰¾åˆ°æ›´æ–°ï¼Œé»˜è®¤ç»§ç»­æµ‹è¯•
                hypothesis.current_state = SAGEState.DESIGN_TEST
                if self.debug:
                    print(f"   âš ï¸  No update found for H{hypothesis.id}, defaulting to DESIGN_TEST")
        
        finally:
            # Restoreä¹‹å‰çš„current_hypothesis_id
            self.state_machine.current_hypothesis_id = old_current_id
    
    def _execute_design_test_with_immediate_run(self):
        """æ‰§è¡Œè®¾è®¡æµ‹è¯•å¹¶ç«‹å³è¿è¡Œæµ‹è¯•"""
        # 1. ç”ŸæˆDESIGN_TESTçš„prompt
        prompt = self.prompt_generator.generate()
        
        if self.debug:
            print(f"Generated prompt ({len(prompt)} chars)")
        
        # 2. è°ƒç”¨LLMè·å–æµ‹è¯•è®¾è®¡
        llm_output = self._get_llm_response_with_retry(prompt)
        
        # 3. éªŒè¯è¾“å‡º
        is_valid, error_msg = self.output_validator.validate(SAGEState.DESIGN_TEST, llm_output)
        
        if not is_valid:
            if self.debug:
                print(f"âš ï¸  Validation failed: {error_msg}")
            return False
        
        # 4. å¤„ç†è¾“å‡ºå¹¶æ‰§è¡Œæµ‹è¯•
        self._process_output(SAGEState.DESIGN_TEST, llm_output)
        
        # 5. ç›´æ¥æ‰§è¡Œæµ‹è¯•ï¼ˆå¦‚æœåŒ…å«[TOOL] model.runï¼‰
        if "[TOOL] model.run" in llm_output:
            # Extractæµ‹è¯•promptå¹¶æ‰§è¡Œ
            test_prompt = self._extract_test_prompt_from_design(llm_output)
            if test_prompt:
                self._execute_test_immediately(test_prompt)
        
        # 6. çŠ¶æ€è½¬æ¢åˆ°ANALYZE_RESULT
        self.state_machine.transition(SAGEState.ANALYZE_RESULT)
        
        return True
    
    def _extract_test_prompt_from_design(self, design_output: str) -> Optional[str]:
        """ä»è®¾è®¡æµ‹è¯•è¾“å‡ºä¸­æå–æµ‹è¯•prompt"""
        import re
        # æŸ¥æ‰¾ [TOOL] model.run prompt='...' æ¨¡å¼
        # ä½¿ç”¨æ›´æ™ºèƒ½çš„åŒ¹é…ï¼šæ”¯æŒè½¬ä¹‰çš„å•å¼•å·ï¼ŒåŒ¹é…åˆ°è¡Œå°¾æˆ–ä¸‹ä¸€ä¸ªæœªè½¬ä¹‰çš„å•å¼•å·
        # å…ˆå°è¯•åŒ¹é…åˆ°è¡Œå°¾ï¼ˆæ›´å®½æ¾ï¼‰
        pattern1 = r"\[TOOL\]\s+model\.run\s+prompt='(.+?)(?:\n|$)"
        match1 = re.search(pattern1, design_output, re.MULTILINE)
        if match1:
            prompt = match1.group(1).rstrip()
            # ç§»é™¤æœ«å°¾å¯èƒ½çš„å•å¼•å·ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
            if prompt.endswith("'"):
                prompt = prompt[:-1]
            # Processè½¬ä¹‰çš„å•å¼•å·ï¼šå°†\'è¿˜åŸä¸º'
            prompt = prompt.replace("\\'", "'")
            return prompt
        
        # Ifç¬¬ä¸€ç§æ–¹æ³•å¤±è´¥ï¼Œå°è¯•åŸæ¥çš„æ–¹æ³•ï¼ˆå‘åå…¼å®¹ï¼‰
        pattern2 = r"\[TOOL\]\s+model\.run\s+prompt='([^']+)'"
        match2 = re.search(pattern2, design_output)
        if match2:
            return match2.group(1)
        return None
    
    def _execute_test_immediately(self, test_prompt: str, hypothesis_id: Optional[int] = None):
        """ç«‹å³æ‰§è¡Œæµ‹è¯•"""
        if self.debug:
            print(f"ğŸ§ª Executing test: {test_prompt[:100]}...")
        
        # Getå½“å‰è¦æµ‹è¯•çš„å‡è®¾IDï¼ˆä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„hypothesis_idï¼‰
        if hypothesis_id is None:
            if self.state_machine.current_hypothesis_id:
                hypothesis_id = self.state_machine.current_hypothesis_id
            else:
                current_hypothesis = self.state_machine.get_next_hypothesis_to_test()
                hypothesis_id = current_hypothesis.id if current_hypothesis else 1
                self.state_machine.current_hypothesis_id = hypothesis_id
        
        # Executeæµ‹è¯•
        # è½¬ä¹‰promptä¸­çš„å•å¼•å·ï¼Œä»¥ä¾¿åœ¨å‘½ä»¤å­—ç¬¦ä¸²ä¸­æ­£ç¡®ä½¿ç”¨
        escaped_prompt = test_prompt.replace("'", "\\'")
        execution_output = self.experiment_env.execute_experiment(f"[TOOL] model.run prompt='{escaped_prompt}'")
        
        if self.debug:
            print(f"ğŸ“ˆ Test execution result:")
            print(execution_output)
        
        # Parseæµ‹è¯•ç»“æœï¼ˆä¼ å…¥æ­£ç¡®çš„hypothesis_idï¼‰
        test_result = self._parse_test_result(execution_output, hypothesis_id=hypothesis_id)
        if test_result:
            # å­˜å‚¨æµ‹è¯•ç»“æœï¼ˆadd_test_resultä¼šè‡ªåŠ¨æ·»åŠ åˆ°å‡è®¾çš„æµ‹è¯•å†å²ï¼‰
            self.state_machine.add_test_result(
                hypothesis_id=test_result.hypothesis_id,
                prompt=test_result.prompt,
                expected=test_result.expected,
                actual_activation=test_result.actual_activation,
                normalized_activation=test_result.normalized_activation,
                result=test_result.result
            )
            
            # Saveå®Œæ•´çš„æµ‹è¯•æ‰§è¡Œè¾“å‡ºåˆ°å‡è®¾å¯¹è±¡ï¼ˆç”¨äºANALYZE_RESULTï¼‰
            hypothesis = self.state_machine.get_hypothesis_by_id(hypothesis_id)
            if hypothesis:
                hypothesis.latest_test_execution_output = execution_output
                # å°†æµ‹è¯•æ‰§è¡Œç»“æœæ·»åŠ åˆ°toolsæ—¥å¿—ä¸­ï¼Œä»¥ä¾¿LLMåœ¨ANALYZE_RESULTæ—¶èƒ½çœ‹åˆ°
                # Addå‡è®¾æ ‡è¯†ï¼Œé¿å…æ··æ·†
                test_output_with_id = f"[HYPOTHESIS {hypothesis_id} TEST RESULT]\n{execution_output}"
                self.tools.update_log(role='system', content=test_output_with_id)
            
            if self.debug:
                print(f"ğŸ“Š Parsed test result: prompt='{test_result.prompt}', activation={test_result.actual_activation}, hypothesis_id={test_result.hypothesis_id}")
                print(f"   âœ… Test execution output saved and added to tools log for H{hypothesis_id}")
    
    def _parse_exemplars_from_output(self, tool_output: str) -> List:
        """ä»å·¥å…·è¾“å‡ºä¸­è§£æexemplarsæ•°æ®"""
        exemplars = []
        
        # Parseexemplarsè¾“å‡ºæ ¼å¼
        lines = tool_output.split('\n')
        current_exemplar = None
        
        for line in lines:
            line = line.strip()
            
            # åŒ¹é…exemplaræ¡ç›®
            if line.startswith(('1.', '2.', '3.', '4.', '5.', '6.', '7.', '8.', '9.', '10.')):
                # Parseæ¿€æ´»å€¼
                if 'max_activation=' in line:
                    try:
                        # Extractæ¿€æ´»å€¼
                        activation_part = line.split('max_activation=')[1].split(',')[0]
                        activation = float(activation_part)
                        
                        # Createç®€åŒ–çš„exemplarå¯¹è±¡
                        exemplar = type('Exemplar', (), {
                            'text': '',
                            'activation': activation,
                            'tokens': [],
                            'per_token_activations': []
                        })()
                        
                        # Addè°ƒè¯•ä¿¡æ¯
                        # if self.debug:
                        #     print(f"ğŸ“Š Created exemplar with activation: {activation}")
                        
                        exemplars.append(exemplar)
                        current_exemplar = exemplar
                    except Exception as e:
                        if self.debug:
                            print(f"âš ï¸  Error parsing activation: {e}")
            
            # åŒ¹é…æ–‡æœ¬å†…å®¹
            elif line.startswith('Text:') and current_exemplar:
                text = line.replace('Text: ', '').strip()
                current_exemplar.text = text
            
            # åŒ¹é…å…³é”®tokens
            elif line.startswith('Key tokens:') and current_exemplar:
                tokens_part = line.replace('Key tokens: ', '').strip()
                # ç®€å•è§£ætokens (æ ¼å¼: 'token':value, 'token':value)
                if tokens_part and tokens_part != 'Token-level analysis not available':
                    try:
                        # Parsetokensæ ¼å¼
                        tokens = []
                        activations = []
                        
                        # ç®€å•è§£æï¼Œå‡è®¾æ ¼å¼ä¸º 'token':value
                        import re
                        matches = re.findall(r"'([^']+)':([0-9.]+)", tokens_part)
                        for token, act in matches:
                            tokens.append(token)
                            activations.append(float(act))
                        
                        current_exemplar.tokens = tokens
                        current_exemplar.per_token_activations = activations
                    except Exception as e:
                        if self.debug:
                            print(f"âš ï¸  Error parsing tokens: {e}")
        
        return exemplars
    
    def _get_llm_response_with_retry(self, prompt: str, max_retries: int = 3) -> str:
        """è·å–LLMå“åº”ï¼Œå¸¦é‡è¯•æœºåˆ¶å’Œä¸Šä¸‹æ–‡å‹ç¼©

        é‡è¯•å»¶è¿Ÿç­–ç•¥ï¼š
        - attempt 1 å¤±è´¥åï¼Œç­‰å¾… 20 ç§’å†å°è¯• attempt 2
        - attempt 2 å¤±è´¥åï¼Œç­‰å¾… 20 ç§’å†å°è¯• attempt 3

        é‡è¦ï¼šåªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶æ·»åŠ  prompt åˆ° logï¼Œé¿å…é‡è¯•æ—¶é‡å¤æ·»åŠ å¯¼è‡´ context è†¨èƒ€
        """
        # åªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶æ·»åŠ  prompt åˆ° log
        prompt_added = False

        for attempt in range(max_retries):
            try:
                # åªåœ¨ç¬¬ä¸€æ¬¡å°è¯•æ—¶æ›´æ–°å·¥å…·æ—¥å¿—ï¼Œé¿å…é‡è¯•æ—¶é‡å¤æ·»åŠ 
                if not prompt_added:
                    self.tools.update_log(role='user', content=prompt)
                    prompt_added = True

                if self.debug:
                    print(f"ğŸ¤– Calling LLM (attempt {attempt + 1}/{max_retries})...")

                # è°ƒç”¨LLM
                response = ask_agent(self.llm_client, self.tools.get_log())
                
                if self.debug:
                    print(f"ğŸ“ LLM Response (length: {len(response)}):")
                    if response:
                        print(response)
                    else:
                        print("(empty response)")
                
                # Checkå“åº”æ˜¯å¦ä¸ºç©ºï¼ˆå¯èƒ½æ˜¯APIé™æµæˆ–è¶…æ—¶ï¼‰
                if not response or len(response.strip()) == 0:
                    if self.debug:
                        print(f"âš ï¸  Empty response on attempt {attempt + 1}/{max_retries} (possible rate limiting or timeout)")
                    
                    if attempt < max_retries - 1:
                        self.execution_stats["retry_attempts"] += 1
                        # ç©ºå“åº”æ—¶ä½¿ç”¨æ›´é•¿çš„å»¶è¿Ÿï¼šattempt 1 å¤±è´¥åç­‰å¾… 20 ç§’ï¼Œattempt 2 å¤±è´¥åç­‰å¾… 20 ç§’
                        if attempt == 0:
                            wait_time = 20
                        elif attempt == 1:
                            wait_time = 20
                        else:
                            wait_time = 20  # é»˜è®¤ 10 ç§’
                        
                        print(f"â³ Waiting {wait_time} seconds before retry (empty response - may be rate limited)...")
                        time.sleep(wait_time)
                        continue
                    else:
                        if self.debug:
                            print("ğŸ”„ Using fallback response due to empty responses")
                        fallback = self._generate_fallback_response()
                        self.tools.update_log(role='assistant', content=fallback)
                        return fallback
                
                # Validateå“åº”è´¨é‡
                if validate_agent_response(response):
                    if self.debug and attempt > 0:
                        print(f"âœ… LLM response successful on attempt {attempt + 1}")

                    # Add assistant çš„å“åº”åˆ° logï¼Œå½¢æˆå®Œæ•´çš„å¯¹è¯å†å²
                    self.tools.update_log(role='assistant', content=response)
                    return response
                else:
                    if self.debug:
                        print(f"âš ï¸  Invalid response on attempt {attempt + 1}/{max_retries}")
                        print(f"   Response:")
                        print(response)
                    
                    if attempt < max_retries - 1:
                        self.execution_stats["retry_attempts"] += 1
                        # Addå»¶è¿Ÿï¼šattempt 1 å¤±è´¥åç­‰å¾… 20 ç§’ï¼Œattempt 2 å¤±è´¥åç­‰å¾… 10 ç§’
                        if attempt == 0:
                            wait_time = 20
                        elif attempt == 1:
                            wait_time = 20
                        else:
                            wait_time = 20  # é»˜è®¤ 10 ç§’
                        
                        print(f"â³ Waiting {wait_time} seconds before retry...")
                        time.sleep(wait_time)
                        continue
                    else:
                        if self.debug:
                            print("ğŸ”„ Using fallback response due to validation failure")
                        fallback = self._generate_fallback_response()
                        self.tools.update_log(role='assistant', content=fallback)
                        return fallback
                        
            except Exception as e:
                if self.debug:
                    print(f"âŒ LLM error on attempt {attempt + 1}: {e}")
                
                # Checkæ˜¯å¦æ˜¯ä¸Šä¸‹æ–‡é•¿åº¦è¶…é™
                error_str = str(e).lower()
                if any(phrase in error_str for phrase in [
                    "context_length_exceeded", 
                    "maximum context length", 
                    "reduce the length of the messages",
                    "8192 tokens"
                ]):
                    if self.debug:
                        print("ğŸ“¦ Context length exceeded, compressing history...")
                    self._compress_context()
                    # é‡æ–°å°è¯•ï¼Œä¸å¢åŠ é‡è¯•è®¡æ•°ï¼Œä¸æ·»åŠ å»¶è¿Ÿï¼ˆä¸Šä¸‹æ–‡å‹ç¼©åç«‹å³é‡è¯•ï¼‰
                    continue
                
                if attempt < max_retries - 1:
                    self.execution_stats["retry_attempts"] += 1
                    # Addå»¶è¿Ÿï¼šattempt 1 å¤±è´¥åç­‰å¾… 20 ç§’ï¼Œattempt 2 å¤±è´¥åç­‰å¾… 10 ç§’
                    if attempt == 0:
                        wait_time = 20
                    elif attempt == 1:
                        wait_time = 20
                    else:
                        wait_time = 20  # é»˜è®¤ 10 ç§’
                    
                    print(f"â³ Waiting {wait_time} seconds before retry...")
                    time.sleep(wait_time)
                    continue
                else:
                    if self.debug:
                        print("ğŸ”„ Using fallback response due to LLM error")
                    fallback = self._generate_fallback_response()
                    self.tools.update_log(role='assistant', content=fallback)
                    return fallback

        if self.debug:
            print("ğŸ”„ Using fallback response after all retries failed")
        fallback = self._generate_fallback_response()
        self.tools.update_log(role='assistant', content=fallback)
        return fallback
    
    def _retry_with_correction(self, original_prompt: str, error_msg: str) -> str:
        """å¸¦é”™è¯¯çº æ­£çš„é‡è¯•
        
        åœ¨è°ƒç”¨çº æ­£é‡è¯•å‰ï¼Œç­‰å¾…ä¸€æ®µæ—¶é—´ä»¥é¿å…APIé™æµ
        """
        correction_prompt = f"""
Your previous output had an error: {error_msg}

Please provide output again following the required format.

{original_prompt}
"""
        
        if self.debug:
            print(f"ğŸ”„ Retrying with correction: {error_msg}")
        
        # åœ¨çº æ­£é‡è¯•å‰ç­‰å¾…ï¼Œå› ä¸ºä¹‹å‰çš„é‡è¯•å¯èƒ½åˆšåˆšå¤±è´¥
        # ç­‰å¾… 15 ç§’ä»¥é¿å…APIé™æµ
        print(f"â³ Waiting 15 seconds before correction retry...")
        time.sleep(15)
        
        return self._get_llm_response_with_retry(correction_prompt, max_retries=2)
    
    def _handle_persistent_error(self, error_msg: str):
        """å¤„ç†æŒç»­é”™è¯¯"""
        if self.debug:
            print(f"âŒ Persistent error: {error_msg}")
        
        self.execution_stats["failed_rounds"] += 1
        
        # According toå½“å‰çŠ¶æ€ç”Ÿæˆé»˜è®¤å“åº”
        fallback_response = self._generate_fallback_response()
        self._process_output(self.state_machine.state, fallback_response)
        
        # ForceçŠ¶æ€è½¬æ¢ï¼Œé¿å…æ— é™å¾ªç¯
        try:
            next_state = self._determine_next_state()
            self.state_machine.transition(next_state)
            if self.debug:
                print(f"ğŸ”„ Forced transition to {next_state.value}")
        except Exception as e:
            if self.debug:
                print(f"âŒ Failed to transition state: {e}")
            # IfçŠ¶æ€è½¬æ¢å¤±è´¥ï¼Œå¼ºåˆ¶ç»“æŸ
            self._force_conclude()
    
    def _generate_fallback_response(self) -> str:
        """ç”Ÿæˆå¤‡ç”¨å“åº”"""
        current_state = self.state_machine.state
        
        if current_state == SAGEState.GET_EXEMPLARS:
            return f"[TOOL] text_exemplars top_k={self.top_k}"
        
        elif current_state == SAGEState.ANALYZE_EXEMPLARS:
            return """
OBSERVATION:
- Pattern 1: [Analysis needed]
- Pattern 2: [Analysis needed]
- Common elements: [Analysis needed]

PRELIMINARY HYPOTHESIS:
This feature requires further analysis.
"""
        
        elif current_state == SAGEState.FORM_HYPOTHESIS:
            return """
[HYPOTHESIS LIST]:
Hypothesis_1: This feature requires systematic testing
Hypothesis_2: This feature may be inactive
Hypothesis_3: This feature needs more data
"""
        
        elif current_state == SAGEState.DESIGN_TEST:
            return """
TESTING HYPOTHESIS: This feature requires systematic testing

TEST DESIGN:
Prompt: 'test input'
Expected: Low activation
Validates: Hypothesis_1

[TOOL] model.run prompt='test input'
"""
        
        elif current_state == SAGEState.ANALYZE_RESULT:
            return """
ANALYSIS:
Summary activation: 0.0
BOS activation: 0.0
Normalized: 0.0
Top non-BOS tokens: []

INTERPRETATION:
Inconclusive result

UPDATED HYPOTHESIS STATUS:
Hypothesis_1: INCONCLUSIVE
"""
        
        elif current_state == SAGEState.FINAL_CONCLUSION:
            return """
[DESCRIPTION]: 
This feature requires further investigation due to insufficient data.

[EVIDENCE]:
- Limited test results available
- Feature behavior unclear

[LABEL 1]: Inconclusive feature
"""
        
        return "Analysis incomplete due to technical issues."
    
    def _simplify_test_output(self, execution_output: str) -> str:
        """ç®€åŒ–æµ‹è¯•è¾“å‡ºï¼Œåªä¿ç•™å…³é”®ä¿¡æ¯"""
        lines = execution_output.split('\n')
        simplified_lines = []
        
        for line in lines:
            if line.startswith('Test prompt:') or line.startswith('Max activation:') or line.startswith('Tokens/activations:'):
                simplified_lines.append(line)
        
        return '\n'.join(simplified_lines) if simplified_lines else execution_output
    
    def _process_output(self, state: SAGEState, output: str):
        """å¤„ç†LLMè¾“å‡º"""
        if state == SAGEState.GET_EXEMPLARS:
            # Executeå·¥å…·è°ƒç”¨
            try:
                if self.debug:
                    print(f"ğŸ”§ Executing tool: {output[:100]}...")
                execution_output = self.experiment_env.execute_experiment(output)
                if execution_output:
                    if self.debug:
                        print(f"ğŸ“Š Tool execution result:")
                        print(execution_output)
                    self.tools.update_log(role='system', content=str(execution_output))
                    # Parseexemplarsæ•°æ®
                    self._parse_exemplars(execution_output)
                else:
                    if self.debug:
                        print("âš ï¸  No tool execution output")
            except Exception as e:
                if self.debug:
                    print(f"âŒ Tool execution error: {e}")
        
        elif state == SAGEState.ANALYZE_EXEMPLARS:
            # Saveåˆ†æç»“æœ
            if self.debug:
                print(f"ğŸ“ Analysis output:")
                print(output)
            self.state_machine.add_analysis(output)
            
            # åŒæ—¶è§£æå‡è®¾ï¼ˆå› ä¸ºRound 2åˆå¹¶äº†åˆ†æä¸å‡è®¾å½¢æˆï¼‰
            if self.debug:
                print(f"ğŸ’¡ Parsing hypotheses from Round 2 analysis:")
            hypotheses = self.output_validator.extract_hypotheses(output)
            for hyp in hypotheses:
                self.state_machine.add_hypothesis(hyp["text"])
                if self.debug:
                    print(f"   Added hypothesis: {hyp['text']}")
        
        elif state == SAGEState.FORM_HYPOTHESIS:
            # Parseå‡è®¾
            if self.debug:
                print(f"ğŸ’¡ Hypothesis formation:")
                print(output)
            hypotheses = self.output_validator.extract_hypotheses(output)
            for hyp in hypotheses:
                self.state_machine.add_hypothesis(hyp["text"])
                if self.debug:
                    print(f"   Added hypothesis: {hyp['text']}")
        
        elif state == SAGEState.DESIGN_TEST:
            # In ... modeï¼ŒDESIGN_TESTçš„è¾“å‡ºå·²ç»åœ¨_process_hypothesis_design_testä¸­å¤„ç†
            # è¿™é‡Œåªå¤„ç†éå¹¶è¡Œæ¨¡å¼çš„æ—§é€»è¾‘
            if self.state_machine.current_hypothesis_id is None:
                # éå¹¶è¡Œæ¨¡å¼ï¼Œæ‰§è¡Œæµ‹è¯•
                try:
                    if self.debug:
                        print(f"ğŸ§ª Executing test: {output[:100]}...")
                    execution_output = self.experiment_env.execute_experiment(output)
                    if execution_output:
                        if self.debug:
                            print(f"ğŸ“ˆ Test execution result:")
                            print(execution_output)
                        
                        # Getå½“å‰è¦æµ‹è¯•çš„å‡è®¾ID
                        current_hypothesis = self.state_machine.get_next_hypothesis_to_test()
                        hypothesis_id = current_hypothesis.id if current_hypothesis else 1
                        
                        # Parseæµ‹è¯•ç»“æœï¼ˆä¼ å…¥æ­£ç¡®çš„hypothesis_idï¼‰
                        test_result = self._parse_test_result(execution_output, hypothesis_id=hypothesis_id)
                        if test_result:
                            # ä½¿ç”¨add_test_resultç¡®ä¿æ·»åŠ åˆ°å‡è®¾çš„æµ‹è¯•å†å²
                            self.state_machine.add_test_result(
                                hypothesis_id=test_result.hypothesis_id,
                                prompt=test_result.prompt,
                                expected=test_result.expected,
                                actual_activation=test_result.actual_activation,
                                normalized_activation=test_result.normalized_activation,
                                result=test_result.result
                            )
                        
                        # ç®€åŒ–è¾“å‡ºä¼ é€’ç»™LLM
                        simplified_output = self._simplify_test_output(execution_output)
                        self.tools.update_log(role='system', content=simplified_output)
                    else:
                        if self.debug:
                            print("âš ï¸  No test execution output")
                except Exception as e:
                    if self.debug:
                        print(f"âŒ Test execution error: {e}")
        
        elif state == SAGEState.ANALYZE_RESULT:
            # Parseåˆ†æç»“æœ
            analysis_result = self.output_validator.extract_analysis_result(output)
            
            # In ... modeï¼Œä½¿ç”¨current_hypothesis_id
            hypothesis_id = self.state_machine.current_hypothesis_id
            if not hypothesis_id and "hypothesis_id" in analysis_result:
                hypothesis_id = analysis_result["hypothesis_id"]
            
            # Addåˆ†æåˆ°å‡è®¾çš„åˆ†æå†å²
            if hypothesis_id:
                self.state_machine.add_analysis(output, hypothesis_id=hypothesis_id)
            
            # Updateå‡è®¾çŠ¶æ€ï¼ˆå¦‚æœæœ‰ï¼‰
            if hypothesis_id and "status" in analysis_result:
                self.state_machine.update_hypothesis(
                    hypothesis_id,
                    analysis_result["status"]
                )
        
        elif state == SAGEState.UPDATE_HYPOTHESIS:
            # Processå‡è®¾æ›´æ–°è¾“å‡º
            if self.debug:
                print(f"ğŸ“ Hypothesis update output:")
                print(output)
            
            # In ... modeï¼Œä½¿ç”¨current_hypothesis_id
            hypothesis_id = self.state_machine.current_hypothesis_id
            
            # Saveåˆ°å‡è®¾çš„åˆ†æå†å²
            if hypothesis_id:
                self.state_machine.add_analysis(output, hypothesis_id=hypothesis_id)
            else:
                # Ifæ²¡æœ‰current_hypothesis_idï¼Œä¿å­˜åˆ°å…¨å±€
                self.state_machine.add_analysis(output)
            
            # Parseå‡è®¾çŠ¶æ€æ›´æ–°
            # UPDATE_HYPOTHESIS è¾“å‡ºæ ¼å¼ï¼š
            # HYPOTHESIS UPDATES:
            # - H1 (REFINED/CONFIRMED/REFUTED): ...
            # - H2 (UNCHANGED): ...
            # ...
            
            # Extractå‡è®¾çŠ¶æ€æ›´æ–°
            hypothesis_updates = self._parse_hypothesis_updates(output)
            for hyp_update in hypothesis_updates:
                hyp_id = hyp_update.get("hypothesis_id")
                status = hyp_update.get("status")
                refined_text = hyp_update.get("refined_text")
                
                # In ... modeï¼Œä¼˜å…ˆä½¿ç”¨current_hypothesis_id
                if not hyp_id and hypothesis_id:
                    hyp_id = hypothesis_id
                
                if hyp_id and status:
                    self.state_machine.update_hypothesis(
                        hyp_id,
                        status,
                        refined_text
                    )
                    if self.debug:
                        print(f"   Updated hypothesis {hyp_id}: {status}")
                    
                    # Updateå‡è®¾çš„å½“å‰çŠ¶æ€
                    current_hyp = self.state_machine.get_hypothesis_by_id(hyp_id)
                    if current_hyp:
                        if status in ["CONFIRMED", "REFUTED"]:
                            current_hyp.current_state = None  # æ ‡è®°ä¸ºå·²å®Œæˆ
                            # Ifè¿™æ˜¯å½“å‰æ­£åœ¨å¤„ç†çš„å‡è®¾ï¼Œæ¸…é™¤current_hypothesis_idä»¥ä¾¿é€‰æ‹©ä¸‹ä¸€ä¸ª
                            if self.state_machine.current_hypothesis_id == hyp_id:
                                self.state_machine.current_hypothesis_id = None
                                if self.debug:
                                    print(f"   Hypothesis {hyp_id} finalized, clearing current_hypothesis_id")
                        else:
                            current_hyp.current_state = SAGEState.DESIGN_TEST  # Continueæµ‹è¯•
        
        elif state == SAGEState.REVIEW_ALL_HYPOTHESES:
            # Savereviewç»“æœ
            self.state_machine.hypothesis_review_result = output
            self.state_machine.add_analysis(output)

            # Checkæ˜¯å¦éœ€è¦è¡¥å……æµ‹è¯•
            import re
            need_testing_match = re.search(r'Need more testing:\s*(YES|NO)', output, re.IGNORECASE)
            if need_testing_match and need_testing_match.group(1).upper() == "YES":
                # Parseå»ºè®®çš„æµ‹è¯•
                suggested_tests = self._parse_suggested_tests_from_review(output)
                if suggested_tests and self.debug:
                    print(f"   ğŸ“‹ Parsed {len(suggested_tests)} suggested tests from REVIEW")
                    for test in suggested_tests[:3]:  # æ˜¾ç¤ºå‰3ä¸ª
                        print(f"      - H{test['hypothesis_id']}: {test['prompt'][:60]}...")
                # å­˜å‚¨å»ºè®®çš„æµ‹è¯•ä»¥ä¾¿åç»­æ‰§è¡Œ
                if not hasattr(self.state_machine, 'supplemental_tests'):
                    self.state_machine.supplemental_tests = []
                self.state_machine.supplemental_tests = suggested_tests
        
        elif state == SAGEState.FINAL_CONCLUSION:
            # Saveæœ€ç»ˆç»“è®º
            self.state_machine.add_analysis(output)
    
    def _parse_exemplars(self, execution_output: str):
        """è§£æexemplarsæ•°æ®"""
        # ä½¿ç”¨çœŸå®çš„exemplarsè§£æé€»è¾‘
        exemplars = self._parse_exemplars_from_output(execution_output)
        if exemplars:
            self.state_machine.set_exemplars(exemplars)
            if self.debug:
                print(f"ğŸ“Š Parsed {len(exemplars)} exemplars from execution output")
        else:
            if self.debug:
                print("âš ï¸  No exemplars parsed from execution output")
    
    def _parse_test_result(self, execution_output: str, hypothesis_id: int = 1):
        """è§£ææµ‹è¯•ç»“æœ"""
        # ParseçœŸå®çš„æµ‹è¯•ç»“æœ
        try:
            # Extractæµ‹è¯•æç¤ºï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            prompt = "unknown"
            # æ ¼å¼1: Test prompt: '...'
            prompt_match1 = re.search(r"Test prompt:\s*'([^']+(?:\\'[^']*)*)'", execution_output, re.IGNORECASE)
            if prompt_match1:
                prompt = prompt_match1.group(1).replace("\\'", "'")
            else:
                # æ ¼å¼2: prompt="..."
                prompt_match2 = re.search(r'prompt=["\']([^"\']+(?:\\["\'][^"\']*)*)["\']', execution_output, re.IGNORECASE)
                if prompt_match2:
                    prompt = prompt_match2.group(1).replace('\\"', '"').replace("\\'", "'")
                else:
                    # æ ¼å¼3: ä»TOOLå‘½ä»¤ä¸­æå–
                    prompt_match3 = re.search(r"\[TOOL\]\s+model\.run\s+prompt=['\"]([^'\"]+(?:\\['\"][^'\"]*)*)['\"]", execution_output, re.IGNORECASE)
                    if prompt_match3:
                        prompt = prompt_match3.group(1).replace('\\"', '"').replace("\\'", "'")
            
            # Extractæœ€å¤§æ¿€æ´»å€¼ï¼ˆæ”¯æŒå¤šç§æ ¼å¼ï¼‰
            actual_activation = 0.0
            # æ ¼å¼1: Max activation: 13.8448
            activation_match1 = re.search(r"Max activation:\s*([\d.]+)", execution_output, re.IGNORECASE)
            if activation_match1:
                actual_activation = float(activation_match1.group(1))
            else:
                # æ ¼å¼2: max_activation=9.8140
                activation_match2 = re.search(r"max_activation\s*=\s*([\d.]+)", execution_output, re.IGNORECASE)
                if activation_match2:
                    actual_activation = float(activation_match2.group(1))
            
            # CreateçœŸå®çš„æµ‹è¯•ç»“æœ
            test_result = TestResult(
                id=len(self.state_machine.test_history) + 1,
                hypothesis_id=hypothesis_id,  # ä½¿ç”¨ä¼ å…¥çš„å‡è®¾ID
                prompt=prompt,
                expected="Unknown",  # éœ€è¦ä»è®¾è®¡é˜¶æ®µè·å–
                actual_activation=actual_activation,
                normalized_activation=actual_activation,  # ç®€åŒ–å¤„ç†
                result="INCONCLUSIVE",  # éœ€è¦æ ¹æ®æ¿€æ´»å€¼åˆ¤æ–­
                timestamp=str(self.state_machine.round)
            )
            
            if self.debug:
                print(f"   Parsed test result: prompt='{prompt[:50]}...', activation={actual_activation}")
            
            return test_result
                
        except Exception as e:
            if self.debug:
                print(f"âš ï¸  Error parsing test result: {e}")
                import traceback
                traceback.print_exc()
            # Createé»˜è®¤æµ‹è¯•ç»“æœ
            return TestResult(
                id=len(self.state_machine.test_history) + 1,
                hypothesis_id=hypothesis_id,
                prompt="parsing_failed",
                expected="Unknown",
                actual_activation=0.0,
                normalized_activation=0.0,
                result="ERROR",
                timestamp=str(self.state_machine.round)
            )
    
    def _determine_next_state(self) -> SAGEState:
        """æ ¹æ®å½“å‰çŠ¶æ€å’Œæ¡ä»¶å†³å®šä¸‹ä¸€ä¸ªçŠ¶æ€"""
        current = self.state_machine.state
        
        # å›ºå®šè½¬æ¢
        if current == SAGEState.INIT:
            return SAGEState.GET_EXEMPLARS
        
        if current == SAGEState.GET_EXEMPLARS:
            return SAGEState.ANALYZE_EXEMPLARS
        
        if current == SAGEState.ANALYZE_EXEMPLARS:
            # è¿›å…¥å¹¶è¡Œå‡è®¾æµ‹è¯•
            return SAGEState.PARALLEL_HYPOTHESIS_TESTING
        
        if current == SAGEState.FORM_HYPOTHESIS:
            # ä¿ç•™ç”¨äºå‘åå…¼å®¹ï¼Œå¦‚æœçŠ¶æ€æœºä»è°ƒç”¨æ­¤çŠ¶æ€
            return SAGEState.PARALLEL_HYPOTHESIS_TESTING
        
        if current == SAGEState.PARALLEL_HYPOTHESIS_TESTING:
            # In ... modeï¼Œæ£€æŸ¥æ˜¯å¦æ‰€æœ‰å‡è®¾éƒ½å·²å®Œæˆ
            if self.state_machine.all_hypotheses_finalized():
                return SAGEState.REVIEW_ALL_HYPOTHESES
            # Otherwiseç»§ç»­å¹¶è¡Œå¤„ç†ï¼ˆä¸‹ä¸€è½®ä¼šå¤„ç†æ‰€æœ‰æ´»è·ƒå‡è®¾ï¼‰
            return SAGEState.PARALLEL_HYPOTHESIS_TESTING
        
        # ä»¥ä¸‹çŠ¶æ€è½¬æ¢ä»…ç”¨äºéå¹¶è¡Œæ¨¡å¼çš„æ—§é€»è¾‘
        if current == SAGEState.DESIGN_TEST:
            # In ... modeï¼ŒDESIGN_TESTç”±_process_hypothesis_design_testå¤„ç†
            if self.state_machine.current_hypothesis_id is None:
                return SAGEState.ANALYZE_RESULT
            else:
                return SAGEState.PARALLEL_HYPOTHESIS_TESTING
        
        # æ¡ä»¶è½¬æ¢ï¼ˆç”¨äºéå¹¶è¡Œæ¨¡å¼ï¼‰
        if current == SAGEState.ANALYZE_RESULT:
            # In ... modeï¼ŒANALYZE_RESULTç”±_process_hypothesis_analyze_resultå¤„ç†
            if self.state_machine.current_hypothesis_id is None:
                return SAGEState.UPDATE_HYPOTHESIS
            else:
                return SAGEState.PARALLEL_HYPOTHESIS_TESTING
        
        if current == SAGEState.UPDATE_HYPOTHESIS:
            # In ... modeï¼ŒUPDATE_HYPOTHESISç”±_process_hypothesis_updateå¤„ç†
            # çŠ¶æ€è½¬æ¢ç”±å‡è®¾çš„current_stateç®¡ç†
            if self.state_machine.current_hypothesis_id:
                # å¹¶è¡Œæ¨¡å¼ä¸‹ï¼Œå›åˆ°å¹¶è¡Œå¤„ç†å…¥å£
                return SAGEState.PARALLEL_HYPOTHESIS_TESTING
            else:
                # éå¹¶è¡Œæ¨¡å¼ï¼Œæ£€æŸ¥æ˜¯å¦éœ€è¦ç»§ç»­æµ‹è¯•
                return SAGEState.DESIGN_TEST
        
        if current == SAGEState.REVIEW_ALL_HYPOTHESES:
            # Safety checkï¼šé™åˆ¶REVIEWå¾ªç¯æ¬¡æ•°
            if not hasattr(self.state_machine, 'review_count'):
                self.state_machine.review_count = 0
            self.state_machine.review_count += 1

            # CheckLLMæ˜¯å¦è¦æ±‚è¡¥å……æµ‹è¯•
            if self.state_machine.hypothesis_review_result:
                # Parsereviewç»“æœï¼Œåˆ¤æ–­æ˜¯å¦éœ€è¦è¡¥å……æµ‹è¯•
                import re
                need_testing_match = re.search(r'Need more testing:\s*(YES|NO)',
                                               self.state_machine.hypothesis_review_result,
                                               re.IGNORECASE)
                if need_testing_match:
                    decision = need_testing_match.group(1).upper()
                    if decision == "YES":
                        # Checkæ˜¯å¦è¶…è¿‡REVIEWå¾ªç¯ä¸Šé™
                        if self.state_machine.review_count > 3:
                            if self.debug:
                                print(f"âš ï¸  REVIEW count ({self.state_machine.review_count}) exceeded limit. Proceeding to final conclusion.")
                            return SAGEState.FINAL_CONCLUSION

                        if self.debug:
                            print(f"ğŸ“‹ Review indicates more testing needed (iteration {self.state_machine.review_count}/3), returning to parallel testing")
                        return SAGEState.PARALLEL_HYPOTHESIS_TESTING
                    else:
                        if self.debug:
                            print("âœ… Review indicates sufficient evidence, proceeding to final conclusion")
                        return SAGEState.FINAL_CONCLUSION
                # Fallback: ä½¿ç”¨å…³é”®è¯åŒ¹é…
                elif "need more testing" in self.state_machine.hypothesis_review_result.lower() or \
                     "è¡¥å……æµ‹è¯•" in self.state_machine.hypothesis_review_result or \
                     "additional tests" in self.state_machine.hypothesis_review_result.lower():
                    if self.debug:
                        print("ğŸ“‹ Review indicates more testing needed (keyword match), returning to parallel testing")
                    return SAGEState.PARALLEL_HYPOTHESIS_TESTING
            # é»˜è®¤è¿›å…¥æœ€ç»ˆç»“è®º
            if self.debug:
                print("âœ… Proceeding to final conclusion after review")
            return SAGEState.FINAL_CONCLUSION
        
        if current == SAGEState.FINAL_CONCLUSION:
            # Ifè¾¾åˆ°æœ€å¤§roundï¼Œå¼ºåˆ¶ç»“æŸ
            if self.state_machine.round >= self.state_machine.max_rounds:
                if self.debug:
                    print(f"âš ï¸  Max round ({self.state_machine.max_rounds}) reached. Forcing conclusion.")
                return SAGEState.DONE
            # Validateæ˜¯å¦ç”Ÿæˆäº†æœ‰æ•ˆç»“è®º
            if self._has_valid_conclusion():
                return SAGEState.DONE
            else:
                # Ifæ²¡æœ‰æœ‰æ•ˆç»“è®ºä¸”æœªè¾¾åˆ°æœ€å¤§roundï¼Œä¿æŒåœ¨FINAL_CONCLUSIONçŠ¶æ€é‡è¯•
                if self.debug:
                    print("âš ï¸  No valid conclusion yet, staying in FINAL_CONCLUSION to retry")
                return SAGEState.FINAL_CONCLUSION
        
        raise ValueError(f"Unexpected state: {current}")
    
    def _has_valid_conclusion(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦æœ‰æœ‰æ•ˆçš„æœ€ç»ˆç»“è®º"""
        if not self.state_machine.analysis_history:
            return False
        
        # Checkæœ€æ–°çš„åˆ†ææ˜¯å¦åŒ…å«æœ€ç»ˆç»“è®ºæ ¼å¼
        latest_analysis = self.state_machine.analysis_history[-1]
        
        # Checkæ˜¯å¦åŒ…å«å¿…éœ€çš„ç»“è®ºéƒ¨åˆ†
        required_sections = ["[DESCRIPTION]:", "[EVIDENCE]:", "[LABEL"]
        has_all_sections = all(section in latest_analysis for section in required_sections)
        
        if not has_all_sections:
            if self.debug:
                print("âš ï¸  Missing required conclusion sections")
            return False
        
        # CheckDESCRIPTIONæ˜¯å¦æœ‰å®é™…å†…å®¹
        desc_match = re.search(r"\[DESCRIPTION\]:\s*(.+?)(?=\[|$)", latest_analysis, re.DOTALL)
        if desc_match:
            desc_text = desc_match.group(1).strip()
 
        
        if self.debug:
            print("âœ… Valid conclusion found")
        return True
    
    def _can_draw_conclusion(self) -> bool:
        """æ£€æŸ¥æ˜¯å¦å¯ä»¥å¾—å‡ºç»“è®º

        å…è®¸ä¸¤ç§æƒ…å†µä¸‹å¾—å‡ºç»“è®ºï¼š
        1. æ—©æœŸè½®æ¬¡ï¼ˆ<10è½®ï¼‰ï¼šè‡³å°‘æœ‰ä¸€ä¸ªCONFIRMEDå‡è®¾
        2. åæœŸè½®æ¬¡ï¼ˆâ‰¥10è½®ï¼‰ï¼šæœ‰REFINEDæˆ–CONFIRMEDå‡è®¾å³å¯ï¼ˆå¤„ç†ä¸ä¸€è‡´çš„æ¨¡å¼ï¼‰
        """
        # Checkæ˜¯å¦æœ‰è¶³å¤Ÿçš„æµ‹è¯•æ•°æ®ï¼ˆè‡³å°‘3ä¸ªæµ‹è¯•ï¼‰
        if len(self.state_machine.test_history) < 3:
            if self.debug:
                print(f"âš ï¸  Not enough test data for conclusion: {len(self.state_machine.test_history)}/3")
            return False

        # Checkæ˜¯å¦æœ‰è¶³å¤Ÿçš„åˆ†ææ•°æ®
        if len(self.state_machine.analysis_history) < 2:
            if self.debug:
                print(f"âš ï¸  Not enough analysis data for conclusion: {len(self.state_machine.analysis_history)}/2")
            return False

        # ç»Ÿè®¡å‡è®¾çŠ¶æ€
        confirmed_hypotheses = [h for h in self.state_machine.hypotheses if h.status == "CONFIRMED"]
        refined_hypotheses = [h for h in self.state_machine.hypotheses if h.status == "REFINED"]

        # ç­–ç•¥1ï¼šå¦‚æœæœ‰CONFIRMEDå‡è®¾ï¼Œæ£€æŸ¥corpusè¦†ç›–ç‡
        if len(confirmed_hypotheses) > 0:
            # Safety checkï¼šç¡®ä¿æ‰€æœ‰é«˜æ¿€æ´»corpus tokenséƒ½è¢«æµ‹è¯•è¿‡
            # ï¼ˆè¿™æ˜¯ç¬¬äºŒé“é˜²çº¿ï¼Œç¬¬ä¸€é“é˜²çº¿æ˜¯UPDATE_HYPOTHESIS promptä¸­çš„æ£€æŸ¥ï¼‰
            if self.state_machine.exemplars and self.state_machine.round < 15:
                # æ”¶é›†æ‰€æœ‰activation >= 10.0çš„tokens
                high_activation_tokens = set()
                for exemplar in self.state_machine.exemplars:
                    if hasattr(exemplar, 'tokens') and hasattr(exemplar, 'per_token_activations'):
                        for token, activation in zip(exemplar.tokens, exemplar.per_token_activations):
                            if activation >= 10.0:
                                # æ ‡å‡†åŒ–tokenï¼ˆå»é™¤å‰å¯¼ç©ºæ ¼ï¼Œä¿ç•™'â–'å‰ç¼€ç”¨äºåŒ¹é…ï¼‰
                                clean_token = token.strip()
                                high_activation_tokens.add(clean_token)
                                # åŒæ—¶æ·»åŠ å»æ‰'â–'çš„ç‰ˆæœ¬ä»¥ä¾¿æ›´å®½æ¾çš„åŒ¹é…
                                if clean_token.startswith('â–'):
                                    high_activation_tokens.add(clean_token[1:])

                # æ”¶é›†æ‰€æœ‰æµ‹è¯•è¿‡çš„tokensï¼ˆä»test promptsä¸­æå–ï¼‰
                tested_content = set()
                for test in self.state_machine.test_history:
                    # å°†promptè½¬ä¸ºå°å†™å¹¶æŒ‰ç©ºæ ¼/æ ‡ç‚¹åˆ†å‰²
                    words = test.prompt.lower().replace(',', ' ').replace('.', ' ').replace('!', ' ').replace('?', ' ').split()
                    tested_content.update(words)

                # Checkæ˜¯å¦æœ‰æœªæµ‹è¯•çš„é«˜æ¿€æ´»tokens
                untested_tokens = []
                for token in high_activation_tokens:
                    token_lower = token.lower().strip('â–')
                    if token_lower not in tested_content:
                        untested_tokens.append(token)

                # Ifæœ‰æ˜æ˜¾çš„coverage gap
                if untested_tokens:
                    if self.debug:
                        print(f"âš ï¸  Corpus coverage check: {len(untested_tokens)} high-activation tokens appear untested")
                        print(f"   Untested: {untested_tokens[:5]}")  # æ˜¾ç¤ºå‰5ä¸ª

                    # Round < 12: å»¶è¿Ÿconclusionä»¥ä¾¿æµ‹è¯•æ›´å¤špatterns
                    if self.state_machine.round < 12:
                        if self.debug:
                            print(f"   â†’ Delaying conclusion to allow more testing (round {self.state_machine.round}/12)")
                        return False
                    else:
                        if self.debug:
                            print(f"   â†’ Allowing conclusion at round {self.state_machine.round} (time constraint)")

            if self.debug:
                print(f"âœ… Sufficient data for conclusion: {len(confirmed_hypotheses)} confirmed hypotheses, {len(self.state_machine.test_history)} tests")
            return True

        # ç­–ç•¥2ï¼šåœ¨åæœŸè½®æ¬¡ï¼ˆâ‰¥10è½®ï¼‰ï¼Œå…è®¸ä½¿ç”¨REFINEDå‡è®¾å¾—å‡ºç»“è®º
        # è¿™å¤„ç†äº†ç‰¹å¾æ¨¡å¼ä¸ä¸€è‡´æˆ–éœ€è¦å¤æ‚ä¸Šä¸‹æ–‡çš„æƒ…å†µ
        if self.state_machine.round >= 10:
            if len(refined_hypotheses) > 0:
                if self.debug:
                    print(f"âœ… Allowing conclusion with {len(refined_hypotheses)} refined hypotheses after {self.state_machine.round} rounds (pattern may be unclear)")
                return True
            else:
                if self.debug:
                    print(f"âš ï¸  No confirmed or refined hypotheses after {self.state_machine.round} rounds")
                return False

        # ç­–ç•¥3ï¼šæ—©æœŸè½®æ¬¡ï¼ˆ<10è½®ï¼‰å¿…é¡»æœ‰CONFIRMEDå‡è®¾
        if self.debug:
            print(f"âš ï¸  No confirmed hypotheses for conclusion (round {self.state_machine.round}/10, need CONFIRMED status)")
        return False
    
    def _compress_context(self):
        """å‹ç¼©ä¸Šä¸‹æ–‡å†å²ï¼Œä¿ç•™å…³é”®ä¿¡æ¯"""
        if self.debug:
            print("ğŸ“¦ Compressing context history...")
        
        # å‹ç¼©å·¥å…·æ—¥å¿—
        if hasattr(self.tools, 'compress_log'):
            self.tools.compress_log()
        
        # å‹ç¼©çŠ¶æ€æœºå†å²
        if hasattr(self.state_machine, 'compress_history'):
            self.state_machine.compress_history()
        
        if self.debug:
            print("âœ… Context compressed")
    
    def _force_conclude(self):
        """å¼ºåˆ¶ç»“æŸ"""
        if self.debug:
            print("ğŸ›‘ Forcing conclusion due to timeout or errors")
        
        self.state_machine.force_conclude()
    
    def _compile_results(self) -> Dict[str, Any]:
        """ç¼–è¯‘æœ€ç»ˆç»“æœ"""
        duration = 0
        if self.execution_stats["start_time"] and self.execution_stats["end_time"]:
            duration = self.execution_stats["end_time"] - self.execution_stats["start_time"]
        
        return {
            "feature_id": self.feature_id,
            "layer": self.layer,
            "final_state": self.state_machine.state.value,
            "total_rounds": self.state_machine.round,
            "execution_stats": self.execution_stats,
            "duration_seconds": duration,
            "hypotheses": [
                {
                    "id": h.id,
                    "text": h.text,
                    "status": h.status,
                    "confidence": h.confidence
                }
                for h in self.state_machine.hypotheses
            ],
            "test_results": [
                {
                    "id": t.id,
                    "hypothesis_id": t.hypothesis_id,
                    "prompt": t.prompt,
                    "result": t.result,
                    "activation": t.actual_activation
                }
                for t in self.state_machine.test_history
            ],
            "analysis_history": self.state_machine.analysis_history,
            "state_info": self.state_machine.get_state_info()
        }
    
    def _parse_suggested_tests_from_review(self, review_output: str) -> List[Dict[str, Any]]:
        """ä»REVIEWè¾“å‡ºä¸­è§£æå»ºè®®çš„æµ‹è¯•"""
        import re
        suggested_tests = []

        # Pattern 1: æ ‡å‡†æ ¼å¼ - H1: ... "test sentence"
        # Example: - H1: Test negative control: "She left for Paris."
        test_pattern1 = r'-?\s*H(\d+):[^"]*"([^"]+)"'
        matches1 = re.finditer(test_pattern1, review_output, re.IGNORECASE)

        for match in matches1:
            hyp_id = int(match.group(1))
            test_prompt = match.group(2).strip()

            # Checkæ˜¯å¦ä¸ºæœ‰æ•ˆçš„æµ‹è¯•å¥å­ï¼ˆæ’é™¤å¤ªçŸ­çš„ç‰‡æ®µï¼‰
            if len(test_prompt.split()) >= 3:  # At least3ä¸ªè¯
                suggested_tests.append({
                    'hypothesis_id': hyp_id,
                    'prompt': test_prompt,
                    'source': 'REVIEW suggestion'
                })

        # Pattern 2: å¤‡ç”¨æ ¼å¼ - åœ¨å¥å­ä¸­é—´çš„å¼•å·
        # Example: H1 lacks negative controls for 'for' (e.g., "She left for Paris.")
        if not suggested_tests:
            test_pattern2 = r'H(\d+)[^"]*"([^"]+)"'
            matches2 = re.finditer(test_pattern2, review_output, re.IGNORECASE)

            for match in matches2:
                hyp_id = int(match.group(1))
                test_prompt = match.group(2).strip()

                if len(test_prompt.split()) >= 3:
                    suggested_tests.append({
                        'hypothesis_id': hyp_id,
                        'prompt': test_prompt,
                        'source': 'REVIEW suggestion'
                    })

        # å»é‡ï¼ˆåŒä¸€ä¸ªpromptä¸é‡å¤æµ‹è¯•ï¼‰
        seen_prompts = set()
        unique_tests = []
        for test in suggested_tests:
            if test['prompt'] not in seen_prompts:
                seen_prompts.add(test['prompt'])
                unique_tests.append(test)

        return unique_tests

    def _execute_supplemental_tests(self):
        """æ‰§è¡ŒREVIEWå»ºè®®çš„è¡¥å……æµ‹è¯•"""
        if not hasattr(self.state_machine, 'supplemental_tests'):
            return

        for test in self.state_machine.supplemental_tests:
            hypothesis_id = test['hypothesis_id']
            prompt = test['prompt']

            if self.debug:
                print(f"\nğŸ§ª Supplemental Test for H{hypothesis_id}: {prompt[:60]}...")

            # ç›´æ¥æ‰§è¡Œæµ‹è¯•ï¼ˆç±»ä¼¼_execute_test_immediatelyï¼‰
            try:
                self._execute_test_immediately(prompt, hypothesis_id=hypothesis_id)
            except Exception as e:
                if self.debug:
                    print(f"   âŒ Supplemental test failed: {e}")

    def _parse_hypothesis_updates(self, output: str) -> List[Dict[str, Any]]:
        """è§£æ UPDATE_HYPOTHESIS è¾“å‡ºä¸­çš„å‡è®¾æ›´æ–°"""
        updates = []
        
        # æŸ¥æ‰¾ HYPOTHESIS UPDATES éƒ¨åˆ†
        # æ ¼å¼: - H1 (REFINED/CONFIRMED/REFUTED): ...
        #     - H2 (UNCHANGED): ...
        # æ”¹è¿›ï¼šå¤„ç†å„ç§æ ¼å¼ï¼ŒåŒ…æ‹¬ "H1 (STATUS):" ä¸­çš„å­—é¢"STATUS"å­—ç¬¦ä¸²
        pattern = r'-?\s*H(\d+)\s*\(([A-Z_]+)\):'
        matches = re.finditer(pattern, output)
        
        valid_statuses = {'CONFIRMED', 'REFUTED', 'REFINED', 'UNCHANGED'}
        
        for match in matches:
            hyp_id = int(match.group(1))
            status = match.group(2)
            
            # Skipå­—é¢å­—ç¬¦ä¸²"STATUS"
            if status == "STATUS":
                if self.debug:
                    print(f"âš ï¸  Skipping literal 'STATUS' string for H{hyp_id}, trying to extract actual status...")
                # å°è¯•ä»åç»­æ–‡æœ¬ä¸­æå–å®é™…çŠ¶æ€
                # æŸ¥æ‰¾ "Refined version", "Evidence:", "Not yet tested" ç­‰å…³é”®è¯
                context_start = match.end()
                context = output[context_start:context_start+200]
                
                # Checkæ˜¯å¦æœ‰å®é™…çŠ¶æ€æŒ‡ç¤º
                if re.search(r'\b(CONFIRMED|REFUTED|REFINED|UNCHANGED)\b', context, re.IGNORECASE):
                    status_match = re.search(r'\b(CONFIRMED|REFUTED|REFINED|UNCHANGED)\b', context, re.IGNORECASE)
                    if status_match:
                        status = status_match.group(1).upper()
                        if self.debug:
                            print(f"   â†’ Extracted actual status: {status}")
                else:
                    # Ifæ— æ³•æå–ï¼Œæ ¹æ®ä¸Šä¸‹æ–‡æ¨æ–­
                    if "Not yet tested" in context or "untested" in context.lower():
                        status = "UNCHANGED"
                    elif "Evidence:" in context or "confirmed" in context.lower():
                        status = "CONFIRMED"
                    elif "refined" in context.lower():
                        status = "REFINED"
                    else:
                        if self.debug:
                            print(f"   â†’ Could not determine status, skipping H{hyp_id}")
                        continue
            
            # ValidateçŠ¶æ€æ˜¯å¦æœ‰æ•ˆ
            if status not in valid_statuses:
                if self.debug:
                    print(f"âš ï¸  Invalid status '{status}' for H{hyp_id}, skipping")
                continue
            
            # Extract refined_text (å¦‚æœå­˜åœ¨)
            refined_text = None
            # å°è¯•æŸ¥æ‰¾ Refined version: åé¢ç›´åˆ°ä¸‹ä¸€ä¸ª H æˆ– CONCLUSION
            context_start = match.end()
            refined_match = re.search(
                r'Refined version:\s*(.+?)(?=H\d+|CONCLUSION|$|\n\n)',
                output[context_start:context_start+500],
                re.DOTALL
            )
            if refined_match:
                refined_text = refined_match.group(1).strip()
            
            updates.append({
                "hypothesis_id": hyp_id,
                "status": status,
                "refined_text": refined_text
            })
        
        # Ifæ²¡æœ‰æ‰¾åˆ° H1, H2 æ ¼å¼ï¼Œå°è¯•æŸ¥æ‰¾ UPDATED HYPOTHESIS STATUS
        if not updates:
            status_match = re.search(
                r'UPDATED HYPOTHESIS STATUS:\s*\nHypothesis:\s*([A-Z_]+)',
                output
            )
            if status_match:
                status = status_match.group(1)
                if status in valid_statuses:
                    # é»˜è®¤æ›´æ–°ç¬¬ä¸€ä¸ªå‡è®¾
                    updates.append({
                        "hypothesis_id": 1,
                        "status": status,
                        "refined_text": None
                    })
        
        return updates


# æµ‹è¯•å‡½æ•°
def test_controller():
    """æµ‹è¯•æ§åˆ¶å™¨"""
    print("Testing SAGE Controller...")
    
    # æ¨¡æ‹Ÿä¾èµ–
    class MockLLMClient:
        def __init__(self):
            self.call_count = 0
        
        def call(self, prompt):
            self.call_count += 1
            return f"Mock response {self.call_count}"
    
    class MockTools:
        def __init__(self):
            self.log = []
        
        def update_log(self, role, content):
            self.log.append({"role": role, "content": content})
        
        def get_log(self):
            return self.log
    
    class MockExperimentEnv:
        def execute_experiment(self, command):
            return f"Mock execution result for: {command}"
    
    # Createæ§åˆ¶å™¨
    controller = SAGEController(
        feature_id=0,
        layer=5,
        llm_client=MockLLMClient(),
        tools=MockTools(),
        experiment_env=MockExperimentEnv(),
        debug=True
    )
    
    # è¿è¡Œæ§åˆ¶å™¨
    results = controller.run()
    
    print(f"Controller test completed!")
    print(f"Results: {results}")
    
    return results


if __name__ == "__main__":
    test_controller()
