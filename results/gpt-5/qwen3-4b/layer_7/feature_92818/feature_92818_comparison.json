{
  "configuration": {
    "model_name": "qwen3-4b",
    "sae_path": null,
    "layer": "7-transcoder-hp",
    "feature_index": 92818,
    "llm_model": "gpt-5",
    "num_examples": 10,
    "activation_threshold_original": 8.0,
    "activation_threshold_used": 1.3571,
    "activation_threshold_source": "dynamic_from_api_exemplars",
    "evaluation_method": "neuronpedia_api",
    "generation_evaluation_uses_api": true,
    "prediction_evaluation_uses_api": true,
    "prediction_evaluation_uses_exemplars": true,
    "prediction_evaluation_uses_selected_exemplars": true,
    "prediction_evaluation_activations_from_exemplars": true,
    "prediction_evaluation_uses_global_normalization": true,
    "global_max_activation": 3.422,
    "neuronpedia_model_id": "qwen3-4b",
    "token_usage": {
      "gpt5": {
        "prompt_tokens": 806,
        "completion_tokens": 3768,
        "total_tokens": 4574,
        "cost_input": 0.00050375,
        "cost_output": 0.01884,
        "cost_total": 0.01934375
      },
      "gpt4o": {
        "prompt_tokens": 37779,
        "completion_tokens": 218,
        "total_tokens": 37997,
        "cost_input": 0.04722375,
        "cost_output": 0.00109,
        "cost_total": 0.048313749999999996
      },
      "total": {
        "prompt_tokens": 38585,
        "completion_tokens": 3986,
        "total_tokens": 42571,
        "cost_input": 0.0477275,
        "cost_output": 0.01993,
        "cost_total": 0.0676575
      },
      "pricing": {
        "gpt5": {
          "input_per_1m": 0.625,
          "cached_input_per_1m": 0.0625,
          "output_per_1m": 5.0
        },
        "gpt4o": {
          "input_per_1m": 1.25,
          "output_per_1m": 5.0
        }
      }
    },
    "threshold_calculation": {
      "exemplars_used": 10,
      "max_activations": [
        3.422,
        3.422,
        3.25,
        3.25,
        2.656,
        2.656,
        2.266,
        2.266,
        1.977,
        1.977
      ],
      "average_max_activation": 1.3571,
      "min_max_activation": 1.977,
      "max_max_activation": 3.422
    },
    "prediction_evaluation_selection": {
      "total_exemplars": 39,
      "excluded_top_n": 10,
      "selected_count": 10,
      "selection_distribution": {
        "num_high": 4,
        "num_medium": 3,
        "num_low": 3
      },
      "selected_texts": [
        " a city magistrate that he was facing the charge because he",
        " a city magistrate that he was facing the charge because he",
        " and Lola discover that they are not so",
        " and Lola discover that they are not so",
        " rehab, many said she was abusing drugs and alcohol along",
        " a note stating that he was going to Trappers Lake",
        ". Reynolds testified that she was surprised the suspect called her",
        " as I thought that it was my Gmail where names and",
        "user\n<?php\n/*\n",
        " as I thought that it was my Gmail where names and"
      ],
      "selected_activations": [
        1.703,
        1.703,
        1.711,
        1.711,
        1.438,
        0.828,
        1.438,
        0.812,
        0.0,
        0.812
      ],
      "activation_range": [
        0.0,
        1.711
      ]
    },
    "note": "All activation evaluations use Neuronpedia API. Activation threshold is dynamically calculated from top 10 high-activating API exemplars (average of max activations / 2). Generation Evaluation uses top 10 high-activating exemplars. Prediction Evaluation uses selected exemplars directly from API (excluding top 10), with activation values already available (no API calls needed for activations). model_name and sae_path are kept for record-keeping only and are not used for any calculations."
  },
  "saia": {
    "explanation": {
      "description": "This feature detects finite forms of the English verb “be” (is/was/were/are) and contracted ’s when they occur inside embedded that-clauses following a pronoun (… that [pronoun] be/’s …). Across our tests, activations clustered on the be/’s token in this frame with peaks between 0.4336 and 1.3906, while main-clause be-forms, possessive ’s, and non–be/have linkers remained at 0.0000. There is context variability: “that it is …” yielded 0.0000, and ’s sensitivity depends on rich clause context; overall test activation range: 0.0000–1.3906.",
      "evidence": "- Test 1: \"I think that he is ready.\" → ' is'=0.4336 (supports H1: be in that-clause)\n- Test 5: \"It is claimed that he is buried.\" → second ' is'=1.3047; main-clause ' is'=0.0000 (supports H1: context dependence; main clause off)\n- Test 9: \"They said that it was unfair.\" → ' was'=0.7891 (supports H1: past form in that-clause)\n- Test 10: \"She thought that they were late.\" → ' were'=0.9102 (supports H1: plural past in that-clause)\n- Test 14: \"They think that they are ready.\" → ' are'=0.1426 (supports H1: weaker but >0 in that-clause)\n- Test 19: \"She said that they are here.\" → ' are'=0.5625 (supports H1: replication for are)\n- Test 6: \"We guessed that she's been waiting.\" → \"'s\"=1.3906 (supports H2: ’s activates in rich that-clause)\n- Test 20: \"We think that she's tall.\" → \"'s\"=0.7969 (supports H2: ’s as “is” in that-clause)\n- Test 16: \"They think that she's happy.\" → \"'s\"=0.5547 (supports H2: ’s as “is” in that-clause)\n- Test 21: \"They said that she's gone.\" → \"'s\"=0.2695 (supports H2: ’s as “has” in that-clause)\n- Test 2: \"She's been waiting.\" → \"'s\"=0.0000 (supports H2: minimal clause does not activate)\n- Test 3: \"John's book vanished.\" → possessive \"'s\"=0.0000 (supports H3: possessive off)\n- Test 7: \"She seems happy.\" → ' seems'=0.0000 (supports H3: non–be/have linker off)\n- Test 22: \"He appears tired.\" → ' appears'=0.0000 (supports H3: non–be/have linker off)\n- Test 4: \"She is happy.\" → ' is'=0.0000 (supports H4: main-clause be off)\n- Test 8: \"They are ready.\" → ' are'=0.0000 (supports H4: main-clause be off)\n- Test 13: \"He was late.\" → ' was'=0.0000 (supports H4: main-clause be off)\n- Test 15: \"They said that it is broken.\" → ' is'=0.0000 (contradiction/edge: “that it is …” zero)\n- Test 18: \"They think that it is true.\" → ' is'=0.0000 (contradiction/edge: “that it is …” zero)\n- Test 11: \"We think that he\\'s happy.\" → split \"'s\" tokens 0.0220 (tokenization artifact; weak support for H2)\n- Test 12: \"They said that he\\'s arrived.\" → split \"'s\" tokens 0.0000 (tokenization artifact; neutral)",
      "labels": [
        {
          "number": 1,
          "text": "Embedded that-clause finite BE/’s detector (… that [pronoun] is/was/were/are or ’s …), with main-clause/possessive/linker suppression and context-dependent magnitude"
        }
      ],
      "label1": "Embedded that-clause finite BE/’s detector (… that [pronoun] is/was/were/are or ’s …), with main-clause/possessive/linker suppression and context-dependent magnitude",
      "label2": ""
    },
    "generated_examples": [
      "The witness claimed that he is lying about the alibi.",
      "She insisted that they are ready to sign the agreement.",
      "I remember that you were nervous during the interview.",
      "The coach believes that we are capable of winning tonight.",
      "He admitted that she was wrong about the results.",
      "They reported that he's recovering well after the surgery.",
      "Journalists confirmed that she's already on her flight home.",
      "We suspect that you are involved in the recent breach.",
      "The board noted that they were unprepared for the audit.",
      "His lawyer argued that she is not the sole culprit."
    ],
    "generation_evaluation": {
      "metrics": {
        "total_examples": 10,
        "successful_examples": 2,
        "success_rate": 0.2,
        "avg_max_activation_all": 0.579736328125,
        "avg_mean_activation_all": 0.04841402493990384,
        "avg_max_activation_successful": 1.609375,
        "avg_mean_activation_successful": 0.13932291666666669
      },
      "detailed_results": [
        {
          "example": "The witness claimed that he is lying about the alibi.",
          "max_activation": 0.421875,
          "mean_activation": 0.03245192307692308,
          "success": false,
          "max_token": " is",
          "max_token_idx": 6,
          "tokens": [
            "<|im_end|>",
            "The",
            " witness",
            " claimed",
            " that",
            " he",
            " is",
            " lying",
            " about",
            " the",
            " al",
            "ibi",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0.421875,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "She insisted that they are ready to sign the agreement.",
          "max_activation": 0.375,
          "mean_activation": 0.03125,
          "success": false,
          "max_token": " are",
          "max_token_idx": 5,
          "tokens": [
            "<|im_end|>",
            "She",
            " insisted",
            " that",
            " they",
            " are",
            " ready",
            " to",
            " sign",
            " the",
            " agreement",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0.375,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "I remember that you were nervous during the interview.",
          "max_activation": 0,
          "mean_activation": 0.0,
          "success": false,
          "max_token": "<|im_end|>",
          "max_token_idx": 0,
          "tokens": [
            "<|im_end|>",
            "I",
            " remember",
            " that",
            " you",
            " were",
            " nervous",
            " during",
            " the",
            " interview",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "The coach believes that we are capable of winning tonight.",
          "max_activation": 0.640625,
          "mean_activation": 0.053385416666666664,
          "success": false,
          "max_token": " are",
          "max_token_idx": 6,
          "tokens": [
            "<|im_end|>",
            "The",
            " coach",
            " believes",
            " that",
            " we",
            " are",
            " capable",
            " of",
            " winning",
            " tonight",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0.640625,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "He admitted that she was wrong about the results.",
          "max_activation": 1.375,
          "mean_activation": 0.125,
          "success": true,
          "max_token": " was",
          "max_token_idx": 5,
          "tokens": [
            "<|im_end|>",
            "He",
            " admitted",
            " that",
            " she",
            " was",
            " wrong",
            " about",
            " the",
            " results",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            1.375,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "They reported that he's recovering well after the surgery.",
          "max_activation": 0.09814453125,
          "mean_activation": 0.0081787109375,
          "success": false,
          "max_token": "'s",
          "max_token_idx": 5,
          "tokens": [
            "<|im_end|>",
            "They",
            " reported",
            " that",
            " he",
            "'s",
            " recovering",
            " well",
            " after",
            " the",
            " surgery",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0.09814453125,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "Journalists confirmed that she's already on her flight home.",
          "max_activation": 0.59375,
          "mean_activation": 0.04567307692307692,
          "success": false,
          "max_token": "'s",
          "max_token_idx": 6,
          "tokens": [
            "<|im_end|>",
            "Journal",
            "ists",
            " confirmed",
            " that",
            " she",
            "'s",
            " already",
            " on",
            " her",
            " flight",
            " home",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0.59375,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "We suspect that you are involved in the recent breach.",
          "max_activation": 1.84375,
          "mean_activation": 0.15364583333333334,
          "success": true,
          "max_token": " are",
          "max_token_idx": 5,
          "tokens": [
            "<|im_end|>",
            "We",
            " suspect",
            " that",
            " you",
            " are",
            " involved",
            " in",
            " the",
            " recent",
            " breach",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            1.84375,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "The board noted that they were unprepared for the audit.",
          "max_activation": 0.44921875,
          "mean_activation": 0.034555288461538464,
          "success": false,
          "max_token": " were",
          "max_token_idx": 6,
          "tokens": [
            "<|im_end|>",
            "The",
            " board",
            " noted",
            " that",
            " they",
            " were",
            " un",
            "prepared",
            " for",
            " the",
            " audit",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0.44921875,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "His lawyer argued that she is not the sole culprit.",
          "max_activation": 0,
          "mean_activation": 0.0,
          "success": false,
          "max_token": "<|im_end|>",
          "max_token_idx": 0,
          "tokens": [
            "<|im_end|>",
            "His",
            " lawyer",
            " argued",
            " that",
            " she",
            " is",
            " not",
            " the",
            " sole",
            " culprit",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        }
      ]
    },
    "prediction_evaluation": {
      "correlation": 0.31722348550588686,
      "p_value": 0.0007766790657273896,
      "predictions": [
        1.26372918491279e-05,
        7.814628139307287e-05,
        3.1100121308093694e-05,
        3.4614516848125826e-05,
        0.00010135588784318328,
        0.004522799176170895,
        9.126645764311226,
        8.211420910176257,
        2.8874975445450928,
        8.302391516665791,
        0.18760000154485307,
        6.558146089612526,
        1.26372918491279e-05,
        6.607788054205832e-05,
        8.349781776729273e-05,
        5.002137148074758e-05,
        0.00020348297985447477,
        0.004074434936150465,
        9.071459963042724,
        8.163320873522434,
        2.7550428573898835,
        8.591606199026964,
        0.2833989202669804,
        7.141147102819143,
        1.7107682394403768e-05,
        1.3386450436112576e-05,
        0.00013114306115873976,
        8.082547275523246e-05,
        0.0012936331243429877,
        0.02913734666015241,
        7.273112644566986,
        0.24251869930863398,
        0.00015363139319110207,
        1.6551620111756237e-05,
        1.2926722321452644e-05,
        3.3987189010036794e-05,
        0.0002896839418426743,
        0.0023323473534428564,
        0.02913734666015241,
        8.60135955117154,
        3.4718671240216183,
        0.00058947627707218,
        1.6551620111756237e-05,
        0.00039248418769739513,
        2.616000691030758e-05,
        1.3899638331489458e-05,
        0.00014500286343012805,
        0.00011878705114493795,
        5.343282336818651,
        4.953086785462095,
        1.7694947879012277,
        0.0031140026739861692,
        0.003936432536081152,
        0.0002742603581998689,
        1.5753971738693726e-05,
        7.814628139307287e-05,
        5.597756169544364e-05,
        0.0002431705152456578,
        0.0007158268646360858,
        0.005905752645775234,
        8.77485510154699,
        4.478041148668788,
        8.28339942828535,
        8.243031021011584,
        6.928053110603344,
        7.848175660191009,
        1.4144173351156004e-05,
        1.5155048497368132e-05,
        0.00010391122854953936,
        0.00028976861288010174,
        0.004108946575156389,
        0.019375747018754757,
        8.481458153610397,
        8.144674260142258,
        2.8280940909961076,
        2.5217107647913424,
        0.00038403870727906015,
        0.04836496402023081,
        1.7107682394403768e-05,
        0.0009541658840753825,
        0.0005981900507913312,
        0.0025066015203332077,
        0.006642912584331037,
        0.001345690261260398,
        0.1469664734605905,
        0.011894375313075967,
        0.018682824281777352,
        0.0002710280305128973,
        0.00153473823535019,
        0.0005557989100987737,
        7.573680931711117e-06,
        0.00026583880082623693,
        0.009193353175600065,
        0.0023500754363253644,
        0.0002280630374172119,
        0.009780875641247666,
        5.0395241748382815e-05,
        1.26372918491279e-05,
        0.0009541658840753825,
        0.0022269094161441283,
        0.0005287845031940025,
        0.014509757836830423,
        0.001345690261260398,
        0.1869794922926949,
        0.009195530505065365,
        0.008862962129787223,
        0.001536674036451235,
        0.005744169639939483,
        0.0008620806629616152
      ],
      "true_values": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        3.6300043834015194,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        3.6300043834015194,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        5.8217051431911155,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.684906487434249,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        4.29208065458796,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        2.9450978959672702,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        2.9450978959672702,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "num_tokens": 109,
      "num_examples": 10,
      "example_results": [
        {
          "text": " a city magistrate that he was facing the charge because he",
          "tokens": [
            "<|im_end|>",
            " a",
            " city",
            " magistrate",
            " that",
            " he",
            " was",
            " facing",
            " the",
            " charge",
            " because",
            " he"
          ],
          "predicted_values": [
            1.26372918491279e-05,
            7.814628139307287e-05,
            3.1100121308093694e-05,
            3.4614516848125826e-05,
            0.00010135588784318328,
            0.004522799176170895,
            9.126645764311226,
            8.211420910176257,
            2.8874975445450928,
            8.302391516665791,
            0.18760000154485307,
            6.558146089612526
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        },
        {
          "text": " a city magistrate that he was facing the charge because he",
          "tokens": [
            "<|im_end|>",
            " a",
            " city",
            " magistrate",
            " that",
            " he",
            " was",
            " facing",
            " the",
            " charge",
            " because",
            " he"
          ],
          "predicted_values": [
            1.26372918491279e-05,
            6.607788054205832e-05,
            8.349781776729273e-05,
            5.002137148074758e-05,
            0.00020348297985447477,
            0.004074434936150465,
            9.071459963042724,
            8.163320873522434,
            2.7550428573898835,
            8.591606199026964,
            0.2833989202669804,
            7.141147102819143
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        },
        {
          "text": " and Lola discover that they are not so",
          "tokens": [
            "<|im_end|>",
            " and",
            " Lola",
            " discover",
            " that",
            " they",
            " are",
            " not",
            " so"
          ],
          "predicted_values": [
            1.7107682394403768e-05,
            1.3386450436112576e-05,
            0.00013114306115873976,
            8.082547275523246e-05,
            0.0012936331243429877,
            0.02913734666015241,
            7.273112644566986,
            0.24251869930863398,
            0.00015363139319110207
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            3.6300043834015194,
            0.0,
            0.0
          ],
          "num_tokens": 9
        },
        {
          "text": " and Lola discover that they are not so",
          "tokens": [
            "<|im_end|>",
            " and",
            " Lola",
            " discover",
            " that",
            " they",
            " are",
            " not",
            " so"
          ],
          "predicted_values": [
            1.6551620111756237e-05,
            1.2926722321452644e-05,
            3.3987189010036794e-05,
            0.0002896839418426743,
            0.0023323473534428564,
            0.02913734666015241,
            8.60135955117154,
            3.4718671240216183,
            0.00058947627707218
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            3.6300043834015194,
            0.0,
            0.0
          ],
          "num_tokens": 9
        },
        {
          "text": " rehab, many said she was abusing drugs and alcohol along",
          "tokens": [
            "<|im_end|>",
            " rehab",
            ",",
            " many",
            " said",
            " she",
            " was",
            " abusing",
            " drugs",
            " and",
            " alcohol",
            " along"
          ],
          "predicted_values": [
            1.6551620111756237e-05,
            0.00039248418769739513,
            2.616000691030758e-05,
            1.3899638331489458e-05,
            0.00014500286343012805,
            0.00011878705114493795,
            5.343282336818651,
            4.953086785462095,
            1.7694947879012277,
            0.0031140026739861692,
            0.003936432536081152,
            0.0002742603581998689
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            5.8217051431911155,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        },
        {
          "text": " a note stating that he was going to Trappers Lake",
          "tokens": [
            "<|im_end|>",
            " a",
            " note",
            " stating",
            " that",
            " he",
            " was",
            " going",
            " to",
            " Tr",
            "appers",
            " Lake"
          ],
          "predicted_values": [
            1.5753971738693726e-05,
            7.814628139307287e-05,
            5.597756169544364e-05,
            0.0002431705152456578,
            0.0007158268646360858,
            0.005905752645775234,
            8.77485510154699,
            4.478041148668788,
            8.28339942828535,
            8.243031021011584,
            6.928053110603344,
            7.848175660191009
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.684906487434249,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        },
        {
          "text": ". Reynolds testified that she was surprised the suspect called her",
          "tokens": [
            "<|im_end|>",
            ".",
            " Reynolds",
            " testified",
            " that",
            " she",
            " was",
            " surprised",
            " the",
            " suspect",
            " called",
            " her"
          ],
          "predicted_values": [
            1.4144173351156004e-05,
            1.5155048497368132e-05,
            0.00010391122854953936,
            0.00028976861288010174,
            0.004108946575156389,
            0.019375747018754757,
            8.481458153610397,
            8.144674260142258,
            2.8280940909961076,
            2.5217107647913424,
            0.00038403870727906015,
            0.04836496402023081
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            4.29208065458796,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        },
        {
          "text": " as I thought that it was my Gmail where names and",
          "tokens": [
            "<|im_end|>",
            " as",
            " I",
            " thought",
            " that",
            " it",
            " was",
            " my",
            " Gmail",
            " where",
            " names",
            " and"
          ],
          "predicted_values": [
            1.7107682394403768e-05,
            0.0009541658840753825,
            0.0005981900507913312,
            0.0025066015203332077,
            0.006642912584331037,
            0.001345690261260398,
            0.1469664734605905,
            0.011894375313075967,
            0.018682824281777352,
            0.0002710280305128973,
            0.00153473823535019,
            0.0005557989100987737
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            2.9450978959672702,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        },
        {
          "text": "user\n<?php\n/*\n",
          "tokens": [
            "<|im_end|>",
            "user",
            "\n",
            "<?",
            "php",
            "\n",
            "/*\n"
          ],
          "predicted_values": [
            7.573680931711117e-06,
            0.00026583880082623693,
            0.009193353175600065,
            0.0023500754363253644,
            0.0002280630374172119,
            0.009780875641247666,
            5.0395241748382815e-05
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 7
        },
        {
          "text": " as I thought that it was my Gmail where names and",
          "tokens": [
            "<|im_end|>",
            " as",
            " I",
            " thought",
            " that",
            " it",
            " was",
            " my",
            " Gmail",
            " where",
            " names",
            " and"
          ],
          "predicted_values": [
            1.26372918491279e-05,
            0.0009541658840753825,
            0.0022269094161441283,
            0.0005287845031940025,
            0.014509757836830423,
            0.001345690261260398,
            0.1869794922926949,
            0.009195530505065365,
            0.008862962129787223,
            0.001536674036451235,
            0.005744169639939483,
            0.0008620806629616152
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            2.9450978959672702,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        }
      ],
      "skipped": false,
      "correlation_valid": true,
      "prediction_variance": 8.974203472638468,
      "actual_variance": 0.8369031872508269,
      "method": "logprobs_token_level",
      "token_usage": {
        "prompt_tokens": 26138,
        "completion_tokens": 109,
        "total_tokens": 26247,
        "cost_input": 0.0326725,
        "cost_output": 0.000545,
        "cost_total": 0.0332175
      },
      "activation_source": "neuronpedia_api",
      "api_config": {
        "model_id": "qwen3-4b",
        "layer": "7-transcoder-hp",
        "feature_index": 92818
      }
    }
  },
  "neuronpedia": {
    "explanation": {
      "description": "uses of the copular/auxiliary verb “be” linking a subject to its predicate or forming passives.",
      "label1": "",
      "label2": ""
    },
    "raw_api_response": {
      "explanation": {
        "id": "cmi3pga110012ztfcbt4m8kag",
        "modelId": "qwen3-4b",
        "layer": "7-transcoder-hp",
        "index": "92818",
        "description": "uses of the copular/auxiliary verb “be” linking a subject to its predicate or forming passives.",
        "authorId": "cmgfd4ew3004cnf0ds8gk5ogz",
        "triggeredByUserId": "cmgfd4ew3004cnf0ds8gk5ogz",
        "notes": null,
        "scoreV1": 0,
        "scoreV2": null,
        "umap_x": 0,
        "umap_y": 0,
        "umap_cluster": 0,
        "umap_log_feature_sparsity": 0,
        "typeName": "oai_token-act-pair",
        "explanationModelName": "gpt-5",
        "createdAt": "2025-11-17T22:17:20.552Z",
        "updatedAt": "2025-11-17T22:17:20.552Z"
      },
      "source": "new"
    },
    "explanation_source": "new",
    "explanation_id": "cmi3pga110012ztfcbt4m8kag",
    "all_available_explanations": [],
    "generated_examples": [
      "The sky is blue today.",
      "My sister is a brilliant doctor.",
      "The keys are on the kitchen table.",
      "The report is reviewed by experts annually.",
      "The castle was built in the thirteenth century.",
      "The road is being repaired after the storm.",
      "The samples have been tested for contamination.",
      "You should be careful around that machinery.",
      "It was Maria who fixed the printer.",
      "Their plan is to be announced tomorrow."
    ],
    "generation_evaluation": {
      "metrics": {
        "total_examples": 10,
        "successful_examples": 0,
        "success_rate": 0.0,
        "avg_max_activation_all": 0.0,
        "avg_mean_activation_all": 0.0,
        "avg_max_activation_successful": 0.0,
        "avg_mean_activation_successful": 0.0
      },
      "detailed_results": [
        {
          "example": "The sky is blue today.",
          "max_activation": 0,
          "mean_activation": 0.0,
          "success": false,
          "max_token": "<|im_end|>",
          "max_token_idx": 0,
          "tokens": [
            "<|im_end|>",
            "The",
            " sky",
            " is",
            " blue",
            " today",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "My sister is a brilliant doctor.",
          "max_activation": 0,
          "mean_activation": 0.0,
          "success": false,
          "max_token": "<|im_end|>",
          "max_token_idx": 0,
          "tokens": [
            "<|im_end|>",
            "My",
            " sister",
            " is",
            " a",
            " brilliant",
            " doctor",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "The keys are on the kitchen table.",
          "max_activation": 0,
          "mean_activation": 0.0,
          "success": false,
          "max_token": "<|im_end|>",
          "max_token_idx": 0,
          "tokens": [
            "<|im_end|>",
            "The",
            " keys",
            " are",
            " on",
            " the",
            " kitchen",
            " table",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "The report is reviewed by experts annually.",
          "max_activation": 0,
          "mean_activation": 0.0,
          "success": false,
          "max_token": "<|im_end|>",
          "max_token_idx": 0,
          "tokens": [
            "<|im_end|>",
            "The",
            " report",
            " is",
            " reviewed",
            " by",
            " experts",
            " annually",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "The castle was built in the thirteenth century.",
          "max_activation": 0,
          "mean_activation": 0.0,
          "success": false,
          "max_token": "<|im_end|>",
          "max_token_idx": 0,
          "tokens": [
            "<|im_end|>",
            "The",
            " castle",
            " was",
            " built",
            " in",
            " the",
            " th",
            "ir",
            "teenth",
            " century",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "The road is being repaired after the storm.",
          "max_activation": 0,
          "mean_activation": 0.0,
          "success": false,
          "max_token": "<|im_end|>",
          "max_token_idx": 0,
          "tokens": [
            "<|im_end|>",
            "The",
            " road",
            " is",
            " being",
            " repaired",
            " after",
            " the",
            " storm",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "The samples have been tested for contamination.",
          "max_activation": 0,
          "mean_activation": 0.0,
          "success": false,
          "max_token": "<|im_end|>",
          "max_token_idx": 0,
          "tokens": [
            "<|im_end|>",
            "The",
            " samples",
            " have",
            " been",
            " tested",
            " for",
            " contamination",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "You should be careful around that machinery.",
          "max_activation": 0,
          "mean_activation": 0.0,
          "success": false,
          "max_token": "<|im_end|>",
          "max_token_idx": 0,
          "tokens": [
            "<|im_end|>",
            "You",
            " should",
            " be",
            " careful",
            " around",
            " that",
            " machinery",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "It was Maria who fixed the printer.",
          "max_activation": 0,
          "mean_activation": 0.0,
          "success": false,
          "max_token": "<|im_end|>",
          "max_token_idx": 0,
          "tokens": [
            "<|im_end|>",
            "It",
            " was",
            " Maria",
            " who",
            " fixed",
            " the",
            " printer",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        },
        {
          "example": "Their plan is to be announced tomorrow.",
          "max_activation": 0,
          "mean_activation": 0.0,
          "success": false,
          "max_token": "<|im_end|>",
          "max_token_idx": 0,
          "tokens": [
            "<|im_end|>",
            "Their",
            " plan",
            " is",
            " to",
            " be",
            " announced",
            " tomorrow",
            "."
          ],
          "per_token_activations": [
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0,
            0
          ]
        }
      ]
    },
    "prediction_evaluation": {
      "correlation": 0.6349451044249181,
      "p_value": 1.2200157765995356e-13,
      "predictions": [
        0.22561778842290023,
        0.06199175262601694,
        0.06464815478540623,
        0.10777639255467215,
        0.29228974954845266,
        0.5105508797311779,
        8.924622044810507,
        5.3320010772093935,
        0.3413763576396123,
        1.4973295345335988,
        1.6497917514989664,
        5.588296622589218,
        0.2200483824218712,
        1.6998898832191638,
        0.06229991849860955,
        0.09546049302901394,
        0.04220681944966699,
        0.15587525457482299,
        8.835511558792852,
        5.3320010772093935,
        0.38345870625701317,
        3.218304296499803,
        1.6497917514989664,
        6.036721455751298,
        0.039662393987324095,
        0.5524689853307782,
        0.07167726326352641,
        0.08995714415201408,
        0.17528246447675558,
        0.19496028752905184,
        8.176004602042386,
        7.053041795127216,
        1.0015644664819119,
        0.039662393987324095,
        0.15545414128250773,
        0.07167726326352641,
        0.08910734429976998,
        0.17528246447675558,
        0.19496028752905184,
        8.34954600138941,
        7.466138406130411,
        1.517422518100298,
        0.0025937535095393923,
        0.16086248992959226,
        0.2789066031941634,
        0.045480435870832066,
        1.7269641530542126,
        0.23893422794014127,
        9.759095433450161,
        4.54363748575928,
        4.642485538656815,
        0.3700831420606233,
        3.1353480641799116,
        0.019388535628485703,
        0.039662393987324095,
        0.15967114944525534,
        0.06525797851274079,
        0.5143564228469175,
        1.4113554938182264,
        0.35961384053573575,
        9.601242551098592,
        0.8300638905318356,
        3.933897362300297,
        1.602117052340434,
        0.11253178763590505,
        0.036098854924658544,
        0.002896868729323237,
        0.1478098770949209,
        0.010206264614446492,
        0.07749038459127312,
        0.21406568261566114,
        1.2625791423269963,
        9.163663451345968,
        7.541852944261529,
        0.22333837972325965,
        2.0237831728885642,
        1.1657651413438928,
        2.8453217556348287,
        0.03982526651725562,
        0.8187326370875176,
        0.40723404530476237,
        0.6668579343304156,
        0.31389057030607576,
        0.7489089102190545,
        9.123973014480137,
        0.1835639200287325,
        0.5357534251770134,
        0.17158281274074494,
        0.20889090968331375,
        0.17768633139465445,
        0.03982526651725562,
        2.195170185375182,
        3.7754439037545735,
        2.293504727027806,
        0.17235016951366744,
        1.6810849891487383,
        0.4555857363004957,
        0.03982526651725562,
        0.8187326370875176,
        0.06435160119329432,
        0.6668579343304156,
        0.7974515068214759,
        0.74989768301018,
        9.343582227418045,
        0.33376428802833036,
        0.8021312300559139,
        0.1688456573829456,
        0.2284906172074792,
        0.1743928145702199
      ],
      "true_values": [
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        3.6300043834015194,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        3.6300043834015194,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        5.8217051431911155,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.684906487434249,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        4.29208065458796,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        2.9450978959672702,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0,
        2.9450978959672702,
        0.0,
        0.0,
        0.0,
        0.0,
        0.0
      ],
      "num_tokens": 109,
      "num_examples": 10,
      "example_results": [
        {
          "text": " a city magistrate that he was facing the charge because he",
          "tokens": [
            "<|im_end|>",
            " a",
            " city",
            " magistrate",
            " that",
            " he",
            " was",
            " facing",
            " the",
            " charge",
            " because",
            " he"
          ],
          "predicted_values": [
            0.22561778842290023,
            0.06199175262601694,
            0.06464815478540623,
            0.10777639255467215,
            0.29228974954845266,
            0.5105508797311779,
            8.924622044810507,
            5.3320010772093935,
            0.3413763576396123,
            1.4973295345335988,
            1.6497917514989664,
            5.588296622589218
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        },
        {
          "text": " a city magistrate that he was facing the charge because he",
          "tokens": [
            "<|im_end|>",
            " a",
            " city",
            " magistrate",
            " that",
            " he",
            " was",
            " facing",
            " the",
            " charge",
            " because",
            " he"
          ],
          "predicted_values": [
            0.2200483824218712,
            1.6998898832191638,
            0.06229991849860955,
            0.09546049302901394,
            0.04220681944966699,
            0.15587525457482299,
            8.835511558792852,
            5.3320010772093935,
            0.38345870625701317,
            3.218304296499803,
            1.6497917514989664,
            6.036721455751298
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        },
        {
          "text": " and Lola discover that they are not so",
          "tokens": [
            "<|im_end|>",
            " and",
            " Lola",
            " discover",
            " that",
            " they",
            " are",
            " not",
            " so"
          ],
          "predicted_values": [
            0.039662393987324095,
            0.5524689853307782,
            0.07167726326352641,
            0.08995714415201408,
            0.17528246447675558,
            0.19496028752905184,
            8.176004602042386,
            7.053041795127216,
            1.0015644664819119
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            3.6300043834015194,
            0.0,
            0.0
          ],
          "num_tokens": 9
        },
        {
          "text": " and Lola discover that they are not so",
          "tokens": [
            "<|im_end|>",
            " and",
            " Lola",
            " discover",
            " that",
            " they",
            " are",
            " not",
            " so"
          ],
          "predicted_values": [
            0.039662393987324095,
            0.15545414128250773,
            0.07167726326352641,
            0.08910734429976998,
            0.17528246447675558,
            0.19496028752905184,
            8.34954600138941,
            7.466138406130411,
            1.517422518100298
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            3.6300043834015194,
            0.0,
            0.0
          ],
          "num_tokens": 9
        },
        {
          "text": " rehab, many said she was abusing drugs and alcohol along",
          "tokens": [
            "<|im_end|>",
            " rehab",
            ",",
            " many",
            " said",
            " she",
            " was",
            " abusing",
            " drugs",
            " and",
            " alcohol",
            " along"
          ],
          "predicted_values": [
            0.0025937535095393923,
            0.16086248992959226,
            0.2789066031941634,
            0.045480435870832066,
            1.7269641530542126,
            0.23893422794014127,
            9.759095433450161,
            4.54363748575928,
            4.642485538656815,
            0.3700831420606233,
            3.1353480641799116,
            0.019388535628485703
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            5.8217051431911155,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        },
        {
          "text": " a note stating that he was going to Trappers Lake",
          "tokens": [
            "<|im_end|>",
            " a",
            " note",
            " stating",
            " that",
            " he",
            " was",
            " going",
            " to",
            " Tr",
            "appers",
            " Lake"
          ],
          "predicted_values": [
            0.039662393987324095,
            0.15967114944525534,
            0.06525797851274079,
            0.5143564228469175,
            1.4113554938182264,
            0.35961384053573575,
            9.601242551098592,
            0.8300638905318356,
            3.933897362300297,
            1.602117052340434,
            0.11253178763590505,
            0.036098854924658544
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.684906487434249,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        },
        {
          "text": ". Reynolds testified that she was surprised the suspect called her",
          "tokens": [
            "<|im_end|>",
            ".",
            " Reynolds",
            " testified",
            " that",
            " she",
            " was",
            " surprised",
            " the",
            " suspect",
            " called",
            " her"
          ],
          "predicted_values": [
            0.002896868729323237,
            0.1478098770949209,
            0.010206264614446492,
            0.07749038459127312,
            0.21406568261566114,
            1.2625791423269963,
            9.163663451345968,
            7.541852944261529,
            0.22333837972325965,
            2.0237831728885642,
            1.1657651413438928,
            2.8453217556348287
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            4.29208065458796,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        },
        {
          "text": " as I thought that it was my Gmail where names and",
          "tokens": [
            "<|im_end|>",
            " as",
            " I",
            " thought",
            " that",
            " it",
            " was",
            " my",
            " Gmail",
            " where",
            " names",
            " and"
          ],
          "predicted_values": [
            0.03982526651725562,
            0.8187326370875176,
            0.40723404530476237,
            0.6668579343304156,
            0.31389057030607576,
            0.7489089102190545,
            9.123973014480137,
            0.1835639200287325,
            0.5357534251770134,
            0.17158281274074494,
            0.20889090968331375,
            0.17768633139465445
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            2.9450978959672702,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        },
        {
          "text": "user\n<?php\n/*\n",
          "tokens": [
            "<|im_end|>",
            "user",
            "\n",
            "<?",
            "php",
            "\n",
            "/*\n"
          ],
          "predicted_values": [
            0.03982526651725562,
            2.195170185375182,
            3.7754439037545735,
            2.293504727027806,
            0.17235016951366744,
            1.6810849891487383,
            0.4555857363004957
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 7
        },
        {
          "text": " as I thought that it was my Gmail where names and",
          "tokens": [
            "<|im_end|>",
            " as",
            " I",
            " thought",
            " that",
            " it",
            " was",
            " my",
            " Gmail",
            " where",
            " names",
            " and"
          ],
          "predicted_values": [
            0.03982526651725562,
            0.8187326370875176,
            0.06435160119329432,
            0.6668579343304156,
            0.7974515068214759,
            0.74989768301018,
            9.343582227418045,
            0.33376428802833036,
            0.8021312300559139,
            0.1688456573829456,
            0.2284906172074792,
            0.1743928145702199
          ],
          "actual_values": [
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0,
            2.9450978959672702,
            0.0,
            0.0,
            0.0,
            0.0,
            0.0
          ],
          "num_tokens": 12
        }
      ],
      "skipped": false,
      "correlation_valid": true,
      "prediction_variance": 7.5898151028177585,
      "actual_variance": 0.8369031872508269,
      "method": "logprobs_token_level",
      "token_usage": {
        "prompt_tokens": 11641,
        "completion_tokens": 109,
        "total_tokens": 11750,
        "cost_input": 0.01455125,
        "cost_output": 0.000545,
        "cost_total": 0.01509625
      },
      "activation_source": "neuronpedia_api",
      "api_config": {
        "model_id": "qwen3-4b",
        "layer": "7-transcoder-hp",
        "feature_index": 92818
      }
    }
  },
  "prediction_evaluation": {
    "selected_exemplars": [
      {
        "text": " a city magistrate that he was facing the charge because he",
        "activation": 1.703
      },
      {
        "text": " a city magistrate that he was facing the charge because he",
        "activation": 1.703
      },
      {
        "text": " and Lola discover that they are not so",
        "activation": 1.711
      },
      {
        "text": " and Lola discover that they are not so",
        "activation": 1.711
      },
      {
        "text": " rehab, many said she was abusing drugs and alcohol along",
        "activation": 1.438
      },
      {
        "text": " a note stating that he was going to Trappers Lake",
        "activation": 0.828
      },
      {
        "text": ". Reynolds testified that she was surprised the suspect called her",
        "activation": 1.438
      },
      {
        "text": " as I thought that it was my Gmail where names and",
        "activation": 0.812
      },
      {
        "text": "user\n<?php\n/*\n",
        "activation": 0.0
      },
      {
        "text": " as I thought that it was my Gmail where names and",
        "activation": 0.812
      }
    ],
    "selection_method": "random_selection_from_categorized_exemplars",
    "selection_details": {
      "excluded_top_n": 10,
      "num_high_selected": 4,
      "num_medium_selected": 3,
      "num_low_selected": 3,
      "total_selected": 10
    },
    "note": "Prediction Evaluation uses selected exemplars directly from API (excluding top 10 used for Generation Evaluation). Exemplars are categorized into high, medium, and low activation groups from remaining exemplars, and randomly selected from each group. Activation values are already available from exemplars, so no API calls are needed to get activations. This avoids bias from explanation text and ensures diverse activation levels in evaluation examples."
  },
  "comparison": {
    "generation": {
      "saia": {
        "total_examples": 10,
        "successful_examples": 2,
        "success_rate": 0.2,
        "avg_max_activation_all": 0.579736328125,
        "avg_mean_activation_all": 0.04841402493990384,
        "avg_max_activation_successful": 1.609375,
        "avg_mean_activation_successful": 0.13932291666666669
      },
      "neuronpedia": {
        "total_examples": 10,
        "successful_examples": 0,
        "success_rate": 0.0,
        "avg_max_activation_all": 0.0,
        "avg_mean_activation_all": 0.0,
        "avg_max_activation_successful": 0.0,
        "avg_mean_activation_successful": 0.0
      },
      "differences": {
        "success_rate_diff": 0.2,
        "avg_max_activation_diff": 0.579736328125,
        "avg_max_activation_successful_diff": 1.609375
      },
      "winner": "SAIA"
    },
    "prediction": {
      "saia": {
        "correlation": 0.31722348550588686,
        "p_value": 0.0007766790657273896,
        "correlation_valid": true
      },
      "neuronpedia": {
        "correlation": 0.6349451044249181,
        "p_value": 1.2200157765995356e-13,
        "correlation_valid": true
      },
      "difference": -0.3177216189190313,
      "winner": "Neuronpedia"
    },
    "overall_winner": "SAIA"
  }
}